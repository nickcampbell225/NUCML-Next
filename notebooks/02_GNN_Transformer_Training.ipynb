{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Notebook 02: GNN-Transformer Training\n\n## The Solution: Physics-Informed Deep Learning with Real EXFOR Data\n\n**Learning Objective:** Train a GNN-Transformer model on real experimental data and see smooth, physics-compliant predictions!\n\n### Architecture\n\n```\nReal EXFOR Data â†’ Graph â†’ GNN â†’ Isotope Embeddings â†’ Transformer â†’ Smooth Ïƒ(E)\n```\n\nThis combines:\n1. **GNN**: Learns nuclear topology from Chart of Nuclides (which isotopes are related)\n2. **Transformer**: Learns smooth energy sequences (no staircase effect!)\n3. **Real Data**: IAEA EXFOR experimental measurements with uncertainties"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nsys.path.append('..')\n\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\n\nfrom nucml_next.data import NucmlDataset\nfrom nucml_next.model import GNNTransformerEvaluator, GNNTransformerTrainer\nfrom nucml_next.physics import PhysicsInformedLoss\n\n# Verify EXFOR data exists\nexfor_path = Path('../data/exfor_enriched.parquet')\nif not exfor_path.exists():\n    raise FileNotFoundError(\n        f\"EXFOR data not found at {exfor_path}\\n\"\n        \"Please run: python scripts/ingest_exfor.py --exfor-root <path> --output data/exfor_enriched.parquet\"\n    )\n\nprint(\"âœ“ Imports successful\")\nprint(\"âœ“ EXFOR data found\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Step 1: Initialize Model\n\n**Training Strategy:** Train on the FULL EXFOR database (all isotopes) to learn:\n1. Nuclear topology from the Chart of Nuclides graph\n2. General reaction physics across many isotopes\n3. Transfer patterns that apply to both data-rich (U-235) and data-sparse (Cl-35) cases\n\nThis is the power of GNNs - they can transfer knowledge through graph connections!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# CRITICAL: Use physics-aware DataSelection for scientifically defensible training\n# Train on neutron-induced reactions at reactor energies (proper ML workflow)\n\nfrom nucml_next.data import DataSelection, NucmlDataset\n\nprint(\"Creating physics-aware data selection for GNN training...\")\nprint(\"=\" * 80)\n\n# Training selection: Neutron-induced, reactor energies, all physical reactions\ntraining_selection = DataSelection(\n    # ============================================================================\n    # PROJECTILE SELECTION\n    # ============================================================================\n    projectile='neutron',          # Options: 'neutron' | 'all'\n                                   # 'neutron' = Only neutron-induced reactions (reactor physics)\n                                   # 'all' = All projectiles (n, p, d, Î±, Î³, etc.)\n\n    # ============================================================================\n    # ENERGY RANGE (eV)\n    # ============================================================================\n    energy_min=1e-5,               # Minimum energy in eV (1e-5 = 0.01 eV, thermal neutrons)\n    energy_max=2e7,                # Maximum energy in eV (2e7 = 20 MeV, reactor physics upper bound)\n                                   # Common ranges:\n                                   #   - Thermal: 1e-5 to 1 eV\n                                   #   - Resonance: 1 to 1e4 eV\n                                   #   - Fast: 1e4 to 2e7 eV (20 MeV)\n                                   #   - High energy: up to 1e9 eV (1 GeV)\n\n    # ============================================================================\n    # REACTION (MT) MODE SELECTION\n    # ============================================================================\n    mt_mode='all_physical',        # Options:\n                                   # 'reactor_core'   â†’ Essential for reactor modeling\n                                   #                    (MT 2, 4, 16, 18, 102, 103, 107)\n                                   #\n                                   # 'threshold_only' â†’ Reactions with energy thresholds\n                                   #                    (MT 16, 17, 103, 104, 105, 106, 107)\n                                   #\n                                   # 'fission_details'â†’ Fission breakdown channels\n                                   #                    (MT 18, 19, 20, 21, 38)\n                                   #\n                                   # 'all_physical'   â†’ All MT codes (use exclude_bookkeeping to filter)\n                                   #\n                                   # 'custom'         â†’ Use custom_mt_codes list (see below)\n\n    custom_mt_codes=None,          # Used only when mt_mode='custom'\n                                   # Example: [2, 18, 102]  # Elastic, fission, capture\n                                   # Example: [16, 17, 18]  # (n,2n), (n,3n), fission\n\n    # ============================================================================\n    # EXCLUSION RULES\n    # ============================================================================\n    exclude_bookkeeping=True,      # Exclude MT 0, 1, and MT >= 9000\n                                   # These are arithmetic identities, not physics!\n\n    # ============================================================================\n    # DATA VALIDITY\n    # ============================================================================\n    drop_invalid=True,             # Drop NaN or non-positive cross-sections\n                                   # Essential for log-transform: log(Ïƒ) requires Ïƒ > 0\n\n    # ============================================================================\n    # EVALUATION CONTROLS (Holdout for Extrapolation Testing)\n    # ============================================================================\n    holdout_isotopes=None,         # List of (Z, A) tuples to exclude from training\n                                   # None = Use all data (default for training)\n                                   # Example: [(92, 235)]           # Hold out U-235 only\n                                   # Example: [(92, 235), (17, 35)] # Hold out U-235 and Cl-35\n                                   # Use this to measure TRUE extrapolation capability!\n\n    # ============================================================================\n    # AME2020/NUBASE2020 ENRICHMENT TIER SELECTION\n    # ============================================================================\n    tiers=['A', 'B', 'C', 'D', 'E']  # ðŸŽ¯ CRITICAL: Controls which AME enrichment columns to use\n                                     #\n                                     # Tier A (13 features) - ALWAYS INCLUDED:\n                                     #   - Z, A, N, Energy (nuclear coordinates)\n                                     #   - 9-feature Numerical Particle Vector:\n                                     #     out_n, out_p, out_a, out_g, out_f, out_t, out_h, out_d, is_met\n                                     #\n                                     # Tier B (15 features) - Geometric: âœ… SELECTED\n                                     #   + R_fm (nuclear radius in femtometers)\n                                     #   + kR (dimensionless interaction parameter k*R)\n                                     #   â†’ Enables modeling of nuclear size effects on cross sections\n                                     #   â†’ Critical for optical model physics\n                                     #\n                                     # Tier C (22 features) - Energetics: âœ… SELECTED\n                                     #   + Mass_Excess_MeV (mass excess)\n                                     #   + Binding_Energy_MeV (total binding energy)\n                                     #   + Binding_Per_Nucleon_MeV (B/A)\n                                     #   + S_1n_MeV, S_2n_MeV (neutron separation energies)\n                                     #   + S_1p_MeV, S_2p_MeV (proton separation energies)\n                                     #   â†’ Thresholds for (n,2n), (n,p), other reactions\n                                     #   â†’ Binding energy correlates with capture cross sections\n                                     #\n                                     # Tier D (30 features) - Topological: âœ… SELECTED\n                                     #   + Spin (nuclear angular momentum)\n                                     #   + Parity (nuclear wavefunction symmetry)\n                                     #   + Valence_N, Valence_P (distance to magic numbers)\n                                     #   + P_Factor (pairing: even-even=+1, odd-odd=-1, odd-A=0)\n                                     #   + Shell_Closure_N, Shell_Closure_P (nuclear shell structure)\n                                     #   + N_Magic, P_Magic (magic number indicators)\n                                     #   â†’ Shell model structure for GNN message passing\n                                     #   â†’ Pairing effects on level density\n                                     #\n                                     # Tier E (38 features) - Complete Q-values: âœ… SELECTED\n                                     #   + Q_alpha_MeV (Î±-decay Q-value)\n                                     #   + Q_2beta_minus_MeV (double Î²â» decay)\n                                     #   + Q_ep_MeV (electron capture + Î²âº)\n                                     #   + Q_beta_minus_n_MeV (Î²â» delayed neutron)\n                                     #   + Q_4beta_minus_MeV, Q_da_MeV, Q_pa_MeV, Q_na_MeV\n                                     #   â†’ All reaction Q-values from AME2020 rct1/rct2 files\n                                     #   â†’ Enables thermodynamic consistency in predictions\n                                     #\n                                     # ðŸ“Š FOR GNN-TRANSFORMER TRAINING: Use ALL tiers A + B + C + D + E\n                                     # - Deep models can learn from full physics representation\n                                     # - Transformer attention can discover which features matter\n                                     # - GNN message passing leverages nuclear topology (Tier D)\n                                     # - Complete Q-values (Tier E) ensure energy conservation\n                                     # - Geometric features (Tier B) crucial for optical model\n                                     # - 38 features total = maximum physics information\n                                     #\n                                     # ðŸ§  RATIONALE FOR FULL ENRICHMENT:\n                                     # - GNN-Transformer has capacity to use all features\n                                     # - Physics-informed architecture benefits from complete data\n                                     # - Transfer learning works better with rich representations\n                                     # - Ablation studies can later identify key features\n                                     #\n                                     # ðŸ’¡ NOTE: Pre-enriched Parquet has ALL columns\n                                     # Parquet columnar format only loads tiers you select!\n)\n\nprint(\"Training Selection:\")\nprint(training_selection)\nprint()\n\n# Load FULL dataset for GNN training with physics-aware filtering\nprint(\"=\" * 80)\nprint(\"Loading EXFOR database with predicate pushdown...\")\nprint(\"=\" * 80)\ndataset = NucmlDataset(\n    data_path='../data/exfor_enriched.parquet',\n    mode='graph',  # Graph mode for GNN training\n    selection=training_selection  # Physics-aware selection\n)\n\n# Initialize GNN-Transformer with 8D node features (includes AME2020 enrichment)\nmodel = GNNTransformerEvaluator(\n    node_features=8,  # Z, A, N, N/Z, Mass_Excess, Binding_Energy, Is_Fissile, Is_Stable\n    gnn_embedding_dim=32,\n    gnn_num_layers=3,\n    transformer_num_layers=4,\n)\n\nprint(f\"\\nâœ“ Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\nprint(f\"âœ“ Node features: {8} (with AME2020 enrichment)\")\n\nprint(f\"\\nðŸ“Š Training Data (Top 10 Isotopes):\")\nisotope_counts = dataset.df.groupby(['Z', 'A']).size().sort_values(ascending=False).head(10)\nelement_map = {92: 'U', 17: 'Cl', 94: 'Pu', 26: 'Fe', 8: 'O', 1: 'H',\n               82: 'Pb', 6: 'C', 13: 'Al', 7: 'N', 11: 'Na', 79: 'Au'}\nfor (z, a), count in isotope_counts.items():\n    elem = element_map.get(z, f'Z{z}')\n    print(f\"   â€¢ {elem}-{a:3d}: {count:>8,} measurements\")\n\nprint(f\"\\nðŸ“Š Top 10 Reaction Types (MT codes):\")\nmt_counts = dataset.df['MT'].value_counts().head(10)\nmt_names = {18: 'Fission', 102: '(n,Î³) Capture', 103: '(n,p)', 2: 'Elastic',\n            16: '(n,2n)', 17: '(n,3n)', 4: 'Inelastic', 107: '(n,Î±)'}\nfor mt, count in mt_counts.items():\n    name = mt_names.get(mt, f'MT-{mt}')\n    print(f\"   â€¢ MT {mt:3d} {name:15s}: {count:>8,} measurements\")\n\n# Count our evaluation targets\nu235_count = len(dataset.df[(dataset.df['Z']==92) & (dataset.df['A']==235)])\ncl35_count = len(dataset.df[(dataset.df['Z']==17) & (dataset.df['A']==35)])\n\nprint(f\"\\nâœ“ Total isotopes in graph: {dataset.df.groupby(['Z', 'A']).ngroups} unique Z/A combinations\")\nprint(f\"âœ“ Total reaction types: {dataset.df['MT'].nunique()} unique MT codes\")\nprint(f\"âœ“ Total measurements: {len(dataset.df):,}\")\nprint(f\"\\nâœ“ Evaluation targets:\")\nprint(f\"   â€¢ U-235: {u235_count:,} measurements (data-rich)\")\nprint(f\"   â€¢ Cl-35: {cl35_count:,} measurements (data-sparse)\")\nprint(f\"\\nðŸŽ¯ GNN will learn from neutron-induced reactions across all isotopes!\")\nprint(f\"   Physics-aware selection ensures scientifically defensible training!\")\nprint(f\"   Predicate pushdown reduced load time by filtering at fragment level!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Train with Physics-Informed Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training data\n",
    "trainer = GNNTransformerTrainer(model)\n",
    "train_data = trainer.prepare_training_data(dataset)\n",
    "\n",
    "# Train\n",
    "history = model.train_model(\n",
    "    train_data[:50],  # Use subset for demo\n",
    "    epochs=20,\n",
    "    learning_rate=1e-3,\n",
    ")\n",
    "\n",
    "# Plot training curves\n",
    "model.plot_training_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Step 3: Compare Predictions for Both Isotopes\n\nGNN-Transformer should produce smooth curves for BOTH data-rich (U-235) and data-sparse (Cl-35) scenarios!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create comparative visualization: U-235 vs Cl-35 predictions\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7))\n\n# LEFT: U-235 fission (data-rich)\nenergies_u235 = np.logspace(0, 2, 500)  # 1-100 eV\nisotope_idx_u235 = dataset.graph_builder.isotope_to_idx.get((92, 235))\n\nif isotope_idx_u235 is not None:\n    # Predict\n    gnn_pred_u235 = model.predict_isotope(\n        dataset.graph_builder.build_global_graph(),\n        isotope_idx_u235,\n        energies_u235\n    )\n    \n    # Plot\n    ax1.plot(energies_u235, gnn_pred_u235, 'g-', lw=2.5, label='GNN-Transformer (Smooth!)')\n    ax1.set_xlabel('Energy (eV)', fontsize=12, fontweight='bold')\n    ax1.set_ylabel('Cross Section (barns)', fontsize=12, fontweight='bold')\n    ax1.set_title('U-235 Fission: SMOOTH Predictions (Data-Rich)\\n' + \n                  'GNN learns from extensive measurements',\n                  fontsize=13, fontweight='bold', color='darkblue')\n    ax1.legend(fontsize=11)\n    ax1.set_yscale('log')\n    ax1.grid(True, alpha=0.3)\n    \n    ax1.annotate('âœ“ No staircase!\\nâœ“ Physics-compliant',\n                xy=(50, gnn_pred_u235[250]), xytext=(70, gnn_pred_u235[250]*2),\n                arrowprops=dict(arrowstyle='->', color='green', lw=2),\n                fontsize=10, color='green', fontweight='bold',\n                bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.7))\nelse:\n    ax1.text(0.5, 0.5, 'U-235 not in graph\\n(Check data loading)',\n             ha='center', va='center', transform=ax1.transAxes, fontsize=11)\n    ax1.set_title('U-235 (No Data)', fontsize=13)\n\n# RIGHT: Cl-35 (n,p) (data-sparse)\nenergies_cl35 = np.logspace(6, 7.3, 500)  # 1-20 MeV\nisotope_idx_cl35 = dataset.graph_builder.isotope_to_idx.get((17, 35))\n\nif isotope_idx_cl35 is not None:\n    # Predict\n    gnn_pred_cl35 = model.predict_isotope(\n        dataset.graph_builder.build_global_graph(),\n        isotope_idx_cl35,\n        energies_cl35\n    )\n    \n    # Get ground truth Cl-35 data\n    cl35_data = dataset.df[(dataset.df['Z'] == 17) & \n                           (dataset.df['A'] == 35) & \n                           (dataset.df['MT'] == 103)]\n    \n    # Plot\n    if len(cl35_data) > 0:\n        ax2.scatter(cl35_data['Energy'], cl35_data['CrossSection'],\n                   s=80, c='blue', marker='o', label=f'EXFOR Data ({len(cl35_data)} pts)',\n                   alpha=0.7, zorder=2, edgecolors='black', linewidths=1)\n    \n    ax2.plot(energies_cl35, gnn_pred_cl35, 'g-', lw=2.5, label='GNN-Transformer (Smooth!)', zorder=1)\n    ax2.set_xlabel('Energy (eV)', fontsize=12, fontweight='bold')\n    ax2.set_ylabel('Cross Section (barns)', fontsize=12, fontweight='bold')\n    ax2.set_title('Cl-35 (n,p): SMOOTH Despite Sparse Data!\\n' + \n                  'GNN transfers knowledge from graph structure',\n                  fontsize=13, fontweight='bold', color='darkgreen')\n    ax2.legend(fontsize=11)\n    ax2.set_xscale('log')\n    ax2.grid(True, alpha=0.3)\n    \n    ax2.annotate('âœ“ Smooth interpolation\\nbetween sparse points!',\n                xy=(energies_cl35[250], gnn_pred_cl35[250]), \n                xytext=(energies_cl35[350], gnn_pred_cl35[250]*1.5),\n                arrowprops=dict(arrowstyle='->', color='green', lw=2),\n                fontsize=10, color='green', fontweight='bold',\n                bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.7))\nelse:\n    ax2.text(0.5, 0.5, 'Cl-35 not in graph\\n(Check data loading)',\n             ha='center', va='center', transform=ax2.transAxes, fontsize=11)\n    ax2.set_title('Cl-35 (No Data)', fontsize=13)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nâœ“ SUCCESS: GNN-Transformer produces smooth predictions for BOTH isotopes!\")\nprint(\"=\"*80)\nprint(\"LEFT (U-235 - Data-Rich):\")\nprint(\"  âœ“ No staircase effect\")\nprint(\"  âœ“ Smooth resonance curves\")\nprint(\"  âœ“ Physics-compliant behavior\")\nprint()\nprint(\"RIGHT (Cl-35 - Data-Sparse):\")\nprint(\"  âœ“ Smooth interpolation between sparse measurements\")\nprint(\"  âœ“ GNN transfers knowledge through graph structure\")\nprint(\"  âœ“ Better than classical ML which overfits to sparse points!\")\nprint(\"=\"*80)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### ðŸŽ“ Key Takeaway\n\n> GNN-Transformer learns **smooth** predictions from real EXFOR data that respect physics!\n>\n> **Key improvements over classical ML:**\n> - âœ“ No staircase effect (smooth energy dependence)\n> - âœ“ Learns isotope relationships from Chart of Nuclides\n> - âœ“ Physics-informed loss ensures constraints\n> - âœ“ Trained on real experimental measurements\n> - âœ“ **Transfer learning**: U-235 (data-rich) helps Cl-35 (data-sparse)!\n>\n> **Critical for research:**\n> - Models can interpolate/extrapolate for under-studied isotopes\n> - Reduces need for expensive experimental campaigns\n> - Provides uncertainty quantification to guide new measurements\n>\n> But are they **reactor-accurate** for U-235? â†’ Notebook 03!\n\nContinue to `03_OpenMC_Loop_and_Inference.ipynb` â†’"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}