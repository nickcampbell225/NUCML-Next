{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Notebook 00: Baselines and Limitations\n\n## Rigorous Evaluation of Classical ML for Nuclear Data\n\n**Learning Objective:** Understand *why* classical machine learning fails for nuclear data evaluation through a rigorous, research-grade comparison using optimal hyperparameters.\n\n**Methodology:**\n- Use **Bayesian hyperparameter optimization** (hyperopt) to find optimal configurations\n- Train on **full EXFOR database** (~3.6M measurements) for maximum data availability\n- Evaluate on **data-rich** (U-235) and **data-sparse** (Cl-35) isotopes\n- Compare against physics requirements (smoothness, extrapolation, constraints)\n\n**Focus Isotopes:**\n- **U-235 Fission** (data-rich, well-understood): Critical for nuclear reactors\n- **Cl-35 (n,p)** (data-sparse, research interest): Important for astrophysics and medical applications\n\n### The Problem\n\nNuclear cross sections \u03c3(E) are smooth, continuous functions of energy. They exhibit:\n- **Resonance peaks**: Sharp but smooth features (especially visible in U-235)\n- **Threshold behavior**: \u03c3(E) = 0 for E < E_threshold, then rises smoothly\n- **Physical constraints**: Conservation laws, unitarity, causality\n\n### Why This Matters\n\nA reactor calculation uses millions of cross-section evaluations. If predictions are:\n- **Jagged** \u2192 Unphysical neutron transport\n- **Discontinuous** \u2192 Numerical instabilities\n- **Wrong at key energies** \u2192 Incorrect k_eff (criticality)\n\nThis is the **Validation Paradox**: Low MSE \u2260 Safe Reactor!\n\n**Additional Challenge:** How do models perform when data is sparse (like Cl-35)?\n\n---\n\n## Part 1: Optimized Decision Trees\n\nWe'll use **hyperparameter optimization** to give Decision Trees their best chance. This eliminates the strawman of \"intentionally bad\" parameters and provides a fair, rigorous comparison."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nsys.path.append('..')\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pathlib import Path\n\nfrom nucml_next.data import NucmlDataset\nfrom nucml_next.baselines import XGBoostEvaluator, DecisionTreeEvaluator\n\n# Set plotting style\nplt.style.use('seaborn-v0_8-darkgrid')\nsns.set_palette(\"husl\")\n%matplotlib inline\n\n# Verify EXFOR data exists\nexfor_path = Path('../data/exfor_processed.parquet')\nif not exfor_path.exists():\n    raise FileNotFoundError(\n        f\"EXFOR data not found at {exfor_path}\\n\"\n        \"Please run: python scripts/ingest_exfor.py --x4-db data/x4sqlite1.db --output data/exfor_processed.parquet\"\n    )\n\nprint(\"\u2713 Imports successful\")\nprint(\"\u2713 EXFOR data found\")\nprint(\"Welcome to NUCML-Next: Understanding ML Limitations with Real Nuclear Data\")"
  },
  {
   "cell_type": "markdown",
   "source": "### Configuration: Feature Tiers and Transformations\n\nBefore loading data, configure which features and transformations to use.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================================================\n# USER CONFIGURATION: Feature Tiers and Transformations\n# ============================================================================\n# Change these settings in ONE place instead of scattered throughout the notebook\n\nfrom nucml_next.data.selection import TransformationConfig\n\n# ============================================================================\n# FEATURE TIER SELECTION\n# ============================================================================\n# Choose which AME2020/NUBASE2020 enrichment tiers to include\n#\n# Tier A (13 features) - ALWAYS INCLUDED:\n#   - Z, A, N, Energy (nuclear coordinates)\n#   - 9-feature Numerical Particle Vector:\n#     out_n, out_p, out_a, out_g, out_f, out_t, out_h, out_d, is_met\n#\n# Tier B (+2 features) - Geometric:\n#   + R_fm (nuclear radius)\n#   + kR (dimensionless interaction parameter)\n#\n# Tier C (+7 features) - Energetics: \u2705 RECOMMENDED FOR BASELINES\n#   + Mass_Excess_MeV (mass excess)\n#   + Binding_Energy_MeV (total binding energy)\n#   + Binding_Per_Nucleon_MeV (B/A)\n#   + S_1n_MeV, S_2n_MeV (neutron separation energies)\n#   + S_1p_MeV, S_2p_MeV (proton separation energies)\n#\n# Tier D (+8 features) - Topological:\n#   + Spin, Parity (nuclear structure)\n#   + Valence_N, Valence_P (distance to magic numbers)\n#   + P_Factor (pairing: even-even/odd-odd)\n#   + Shell_Closure_N, Shell_Closure_P\n#\n# Tier E (+8 features) - Complete Q-values:\n#   + Q_alpha_MeV, Q_2beta_minus_MeV, Q_ep_MeV, etc.\n#   + All 8 reaction Q-values from AME2020\n\nSELECTED_TIERS = ['A', 'C']  # \ud83c\udfaf Change tiers HERE (only place to modify)\n\nprint(f\"\ud83d\udcca Selected Feature Tiers: {SELECTED_TIERS}\")\nprint(f\"   Features: Tier A (core + particle vector) + Tier C (energetics)\")\nprint()\n\n# ============================================================================\n# TRANSFORMATION CONFIGURATION\n# ============================================================================\n# Configure log-scaling and standardization for ML training\n#\n# This controls how features and targets are transformed before training.\n# All transformations are reversible for predictions.\n\nTRANSFORMATION_CONFIG = TransformationConfig(\n    # Target (cross-section) transformations\n    log_target=True,              # Enable log\u2081\u2080 transform for cross-sections\n                                  # Stabilizes gradients and handles wide \u03c3 range (mb to kb)\n    \n    target_epsilon=1e-10,         # Epsilon for log(\u03c3 + \u03b5) to prevent log(0)\n                                  # Increase if you have very small cross-sections\n    \n    log_base=10,                  # Logarithm base: 10 | 'e' | 2\n                                  # Base-10 is standard in nuclear physics\n    \n    # Energy transformations\n    log_energy=True,              # Enable log\u2081\u2080 transform for energies\n                                  # Handles wide energy range (eV to MeV)\n    \n    energy_log_base=10,           # Energy log base: 10 | 'e' | 2\n    \n    # Feature standardization\n    scaler_type='standard',       # Feature scaling method:\n                                  # 'standard' = Z-score normalization (X-\u03bc)/\u03c3 [DEFAULT]\n                                  # 'minmax'   = Min-max scaling to [0,1]\n                                  # 'robust'   = Robust scaling using median and IQR\n                                  # 'none'     = No scaling (use raw features)\n    \n    scale_features=None           # Columns to scale. None = auto-detect numeric columns\n                                  # Example: ['Z', 'A', 'N', 'Mass_Excess_MeV']\n)\n\nprint(\"\ud83d\udd27 Transformation Configuration:\")\nprint(TRANSFORMATION_CONFIG)\nprint()\nprint(\"\ud83d\udca1 To change settings, modify SELECTED_TIERS and TRANSFORMATION_CONFIG above\")\nprint(\"=\" * 80)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Step 1.1: Load Real EXFOR Data (Tabular View)\n\nWe'll train on the **full EXFOR database** (all isotopes), then evaluate on U-235 and Cl-35. This is the correct ML approach:\n- **Training**: Learn general nuclear physics patterns from ALL available data\n- **Evaluation**: Test predictions on specific target isotopes (U-235, Cl-35)\n\nThis demonstrates true transfer learning and generalization, not just memorization!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# CRITICAL FIX: Use physics-aware DataSelection for scientifically defensible filtering\n# This demonstrates proper ML workflow with predicate pushdown for efficiency\n\nfrom nucml_next.data import DataSelection\n\n# Strategy: Train on neutron-induced reactions at reactor energies (scientifically defensible default)\n# This avoids training on:\n# - Non-reactor energies (too high/low for criticality calculations)\n# - Non-neutron projectiles (we're focused on reactor physics)\n# \n# NOTE: We now INCLUDE all MT codes (including 0, 1, and >=9000) in 'all_physical' mode\n\nprint(\"Creating physics-aware data selection...\")\nprint(\"=\" * 80)\n\n# Training selection: Reactor physics, neutrons, all physical reactions\ntraining_selection = DataSelection(\n    # ============================================================================\n    # PROJECTILE SELECTION\n    # ============================================================================\n    projectile='neutron',          # Options: 'neutron' | 'all'\n                                   # 'neutron' = Only neutron-induced reactions (reactor physics)\n                                   # 'all' = All projectiles (n, p, d, \u03b1, \u03b3, etc.)\n    \n    # ============================================================================\n    # ENERGY RANGE (eV)\n    # ============================================================================\n    energy_min=1e-5,               # Minimum energy in eV (1e-5 = 0.01 eV, thermal neutrons)\n    energy_max=2e7,                # Maximum energy in eV (2e7 = 20 MeV, reactor physics upper bound)\n                                   # Common ranges:\n                                   #   - Thermal: 1e-5 to 1 eV\n                                   #   - Resonance: 1 to 1e4 eV\n                                   #   - Fast: 1e4 to 2e7 eV (20 MeV)\n                                   #   - High energy: up to 1e9 eV (1 GeV)\n    \n    # ============================================================================\n    # REACTION (MT) MODE SELECTION\n    # ============================================================================\n    mt_mode='all_physical',        # Options:\n                                   # 'reactor_core'   \u2192 Essential for reactor modeling\n                                   #                    (MT 2, 4, 16, 18, 102, 103, 107)\n                                   #                    [elastic, inelastic, (n,2n), fission,\n                                   #                     capture, (n,p), (n,\u03b1)]\n                                   #\n                                   # 'threshold_only' \u2192 Reactions with energy thresholds\n                                   #                    (MT 16, 17, 103, 104, 105, 106, 107)\n                                   #                    [(n,2n), (n,3n), (n,p), (n,d), (n,t),\n                                   #                     (n,\u00b3He), (n,\u03b1)]\n                                   #\n                                   # 'fission_details'\u2192 Fission breakdown channels\n                                   #                    (MT 18, 19, 20, 21, 38)\n                                   #                    [total fission, 1st chance, 2nd chance,\n                                   #                     3rd chance, 4th chance]\n                                   #\n                                   # 'all_physical'   \u2192 All MT codes including bookkeeping\n                                   #                    (MT 0, 1, and >=9000 now INCLUDED)\n                                   #\n                                   # 'custom'         \u2192 Use custom_mt_codes list (see below)\n    \n    custom_mt_codes=None,          # Used only when mt_mode='custom'\n                                   # Example: [2, 18, 102]  # Elastic, fission, capture\n                                   # Example: [16, 17, 18]  # (n,2n), (n,3n), fission\n                                   # Example: list(range(50, 92))  # MT 50-91 (inelastic levels)\n    \n    # ============================================================================\n    # EXCLUSION RULES\n    # ============================================================================\n    exclude_bookkeeping=False,     # Set to False to INCLUDE MT 0, 1, and MT >= 9000\n                                   # MT 0 = Undefined\n                                   # MT 1 = Total cross-section (sum of others)\n                                   # MT >= 9000 = Lumped reaction covariances\n                                   # Now including these for comprehensive analysis\n    \n    # ============================================================================\n    # DATA VALIDITY\n    # ============================================================================\n    drop_invalid=True,             # Drop NaN or non-positive cross-sections\n                                   # Essential for log-transform: log(\u03c3) requires \u03c3 > 0\n                                   # Prevents training instabilities\n    \n    # ============================================================================\n    # EVALUATION CONTROLS (Holdout for Extrapolation Testing)\n    # ============================================================================\n    holdout_isotopes=None,         # List of (Z, A) tuples to exclude from training\n                                   # None = Use all data (default for training)\n                                   # Example: [(92, 235)]           # Hold out U-235 only\n                                   # Example: [(92, 235), (17, 35)] # Hold out U-235 and Cl-35\n                                   # Example: [(94, 239), (94, 240), (94, 241)]  # Pu isotopes\n                                   # Use this to measure TRUE extrapolation capability!\n    \n    # ============================================================================\n    # AME2020/NUBASE2020 ENRICHMENT TIER SELECTION\n    # ============================================================================\n    tiers=SELECTED_TIERS,          # \ud83c\udfaf Using centralized tier configuration\n    transformation_config=TRANSFORMATION_CONFIG  # Using centralized transformation config\n)\n\nprint(\"Training Selection:\")\nprint(training_selection)\nprint()\n\n# Load FULL dataset for training with physics-aware filtering\n# CRITICAL: Predicate pushdown filters at PyArrow fragment level (90% I/O reduction!)\nprint(\"=\" * 80)\nprint(\"Loading training dataset with predicate pushdown...\")\nprint(\"=\" * 80)\ndataset_full = NucmlDataset(\n    data_path='../data/exfor_processed.parquet',\n    mode='tabular',\n    selection=training_selection  # Physics-aware selection with predicate pushdown\n)\n\n# ============================================================================\n# Apply manual energy filter (PyArrow energy filter not working correctly)\n# ============================================================================\n# PyArrow's predicate pushdown is not correctly applying energy filter.\n# Despite specifying energy_max=2e7 (20 MeV), data up to 1 GeV is loaded.\n#\n# TEMPORARY FIX: Apply manual energy filter after loading\n\nprint(\"\\n\u26a0\ufe0f  Applying manual filters (PyArrow bug workarounds)...\")\noriginal_size = len(dataset_full.df)\n\n# Filter: Energy range (should be 1e-5 to 2e7, but PyArrow loads up to 1e9)\ndataset_full.df = dataset_full.df[\n    (dataset_full.df['Energy'] >= training_selection.energy_min) &\n    (dataset_full.df['Energy'] <= training_selection.energy_max)\n].copy()\nafter_energy = len(dataset_full.df)\n\nprint(f\"  Energy filter:     {original_size:,} \u2192 {after_energy:,} ({after_energy/original_size*100:.1f}%)\")\nprint(f\"  Total filtered:    {original_size:,} \u2192 {after_energy:,} ({after_energy/original_size*100:.1f}%)\")\n\n# ============================================================================\n# Project to tabular format with TIER-BASED features (particle vector)\n# ============================================================================\nprint(\"\\n\" + \"=\" * 80)\nprint(\"Projecting to tabular format (tier mode with particle vector)...\")\nprint(\"=\" * 80)\n\n    tiers=SELECTED_TIERS,          # \ud83c\udfaf Using centralized tier configuration\nprint(f\"\\n\u2713 Training dataset: {df_tier.shape}\")\nprint(f\"  Energy range: {df_tier['Energy'].min():.2e} to {df_tier['Energy'].max():.2e} eV\")\nprint(f\"\\n\ud83d\udccb Features Available:\")\nprint(f\"  Tier A features: Z, A, N, Energy + 9-feature particle vector\")\nprint(f\"    Particle vector: out_n, out_p, out_a, out_g, out_f, out_t, out_h, out_d, is_met\")\nprint(f\"  Tier C features: Mass excess, binding, separation energies\")\nprint(f\"  Total features: {len(df_tier.columns)}\")\nprint(f\"\\nFeature names:\")\nprint(df_tier.columns.tolist())\n\n# Show isotope distribution in training data\nprint(\"\\n\ud83d\udcca Training Data Distribution (Top 10 Isotopes):\")\nisotope_counts = dataset_full.df.groupby(['Z', 'A']).size().sort_values(ascending=False).head(10)\nfor (z, a), count in isotope_counts.items():\n    # Simple element lookup (extend as needed)\n    element_map = {92: 'U', 17: 'Cl', 94: 'Pu', 26: 'Fe', 8: 'O', 1: 'H',\n                   82: 'Pb', 6: 'C', 13: 'Al', 7: 'N', 11: 'Na', 79: 'Au'}\n    elem = element_map.get(z, f'Z{z}')\n    print(f\"  {elem}-{a:3d}: {count:>8,} measurements\")\n\nprint(f\"\\n\u2713 Total isotopes: {dataset_full.df.groupby(['Z', 'A']).ngroups} unique Z/A combinations\")\nprint(f\"\u2713 Total reaction types: {dataset_full.df['MT'].nunique()} unique MT codes\")\nprint(f\"\u2713 Total measurements: {len(dataset_full.df):,}\")\n\n# Show MT distribution\nprint(\"\\n\ud83d\udcca Top 10 Reaction Types (MT codes):\")\nmt_counts = dataset_full.df['MT'].value_counts().head(10)\nmt_names = {18: 'Fission', 102: '(n,\u03b3) Capture', 103: '(n,p)', 2: 'Elastic',\n            16: '(n,2n)', 17: '(n,3n)', 4: 'Inelastic', 107: '(n,\u03b1)',\n            0: 'Undefined', 1: 'Total XS'}\nfor mt, count in mt_counts.items():\n    name = mt_names.get(mt, f'MT-{mt}')\n    print(f\"  MT {mt:3d} {name:15s}: {count:>8,} measurements\")\n\nprint(f\"\\n\u2713 Training on neutron-induced reactions (reactor energies 0.01 eV - 20 MeV)\")\nprint(f\"\u2713 Including ALL MT codes (0, 1, and >=9000 now included)\")\nprint(f\"\u2713 Using Tier A + C features with particle vector transformation\")\n\n# Now load evaluation targets (U-235 and Cl-35) using legacy filters for specific selection\nprint(\"\\n\" + \"=\"*70)\nprint(\"Loading evaluation targets (U-235 and Cl-35)...\")\nprint(f\"NOTE: Using same energy range as training: {training_selection.energy_min:.2e} to {training_selection.energy_max:.2e} eV\")\nprint(\"=\"*70)\n\n# Create evaluation selection with same energy limits but specific isotopes\neval_selection = DataSelection(\n    projectile='neutron',\n    energy_min=training_selection.energy_min,  # SAME as training\n    energy_max=training_selection.energy_max,  # SAME as training  \n    mt_mode='custom',\n    custom_mt_codes=[18, 102, 103],  # Fission, capture, (n,p) - for visualization\n    exclude_bookkeeping=False,  # Include all MT codes\n    drop_invalid=True,\n    tiers=SELECTED_TIERS,          # \ud83c\udfaf Using centralized tier configuration\n)\n\ndataset_eval = NucmlDataset(\n    data_path='../data/exfor_processed.parquet',\n    mode='tabular',\n    selection=eval_selection\n)\n\n# Apply same manual energy filter (workaround for PyArrow bugs)\ndataset_eval.df = dataset_eval.df[\n    (dataset_eval.df['Energy'] >= eval_selection.energy_min) &\n    (dataset_eval.df['Energy'] <= eval_selection.energy_max)\n].copy()\n\n# Filter to U-235 and Cl-35 only\ndataset_eval.df = dataset_eval.df[\n    ((dataset_eval.df['Z'] == 92) & (dataset_eval.df['A'] == 235)) |\n    ((dataset_eval.df['Z'] == 17) & (dataset_eval.df['A'] == 35))\n].copy()\n\nprint(f\"\u2713 Evaluation dataset: {len(dataset_eval.df)} measurements\")\nprint(f\"  Energy range: {dataset_eval.df['Energy'].min():.2e} to {dataset_eval.df['Energy'].max():.2e} eV\")\nprint(\"\\n\ud83d\udcca Evaluation Isotopes:\")\nfor (z, a), group in dataset_eval.df.groupby(['Z', 'A']):\n    isotope = f\"{'U' if z==92 else 'Cl'}-{a}\"\n    e_min = group['Energy'].min()\n    e_max = group['Energy'].max()\n    print(f\"  {isotope:8s}: {len(group):>6,} measurements (Energy: {e_min:.2e} to {e_max:.2e} eV)\")\nprint(\"=\"*70)\n\ndf_tier.head()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notice:** The naive approach treats reactions as independent categories (MT_2, MT_18, etc.).\n",
    "\n",
    "**Problem:** This ignores physics! (n,2n) and (n,3n) are related - they differ by one neutron.\n",
    "\n",
    "But tree-based models don't know this. To them, MT=16 and MT=17 are just labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Step 1.2: Optimize and Train Decision Tree\n\nInstead of artificially limiting the model, we'll use **Bayesian hyperparameter optimization** to find the optimal Decision Tree configuration. This is a research-grade approach that gives classical ML its best chance to succeed.\n\n**Research Question**: Even with optimal hyperparameters, can Decision Trees produce smooth, physics-compliant cross-section predictions?"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# HYPERPARAMETER OPTIMIZATION FOR DECISION TREE\n# ============================================================================\n# Use Bayesian optimization (hyperopt) to find optimal hyperparameters\n# This ensures we're evaluating Decision Trees at their BEST, not strawman configs\n\nprint(\"=\"*80)\nprint(\"STEP 1: HYPERPARAMETER OPTIMIZATION\")\nprint(\"=\"*80)\nprint(\"Finding optimal Decision Tree configuration using Bayesian optimization...\")\nprint(\"This may take 5-10 minutes on large datasets.\\n\")\n\n# Initialize temporary model for optimization\ndt_optimizer = DecisionTreeEvaluator()\n\n# Run hyperparameter optimization (50 evaluations for speed, increase for production)\nopt_result_dt = dt_optimizer.optimize_hyperparameters(\n    df_tier,\n    max_evals=50,          # Increase to 100+ for production\n    cv_folds=3,            # 3-fold cross-validation\n    verbose=True\n)\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"STEP 2: TRAINING FINAL MODEL WITH OPTIMAL HYPERPARAMETERS\")\nprint(\"=\"*80)\n\n# Create model with optimal hyperparameters\ndt_model = DecisionTreeEvaluator(**opt_result_dt['best_params'])\n\n# Train on full dataset\ndt_metrics = dt_model.train(df_tier)\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Optimized Decision Tree Performance:\")\nprint(\"=\"*60)\nfor key, value in dt_metrics.items():\n    print(f\"  {key:20s}: {value}\")\n\nprint(\"\\n\ud83d\udca1 Note: This is an OPTIMIZED Decision Tree, not artificially constrained.\")\nprint(\"   The staircase effect is inherent to the algorithm, not poor hyperparameters!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Step 1.3: The Failure Mode - Visualize the Staircase Effect\n\nLet's predict cross sections for both isotopes and see what happens...\n\n**U-235**: Rich data \u2192 Can the model capture resonances?  \n**Cl-35**: Sparse data \u2192 Can the model interpolate gaps?"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# HELPER FUNCTIONS for Clean Cross-Section Plotting\n# ============================================================================\n\ndef _clean_xy(df, col_x='Energy', col_y='CrossSection', emin=None, emax=None):\n    \"\"\"\n    Clean and prepare data for log-log plotting.\n    \n    - Drops NaN values in Energy and CrossSection\n    - Removes non-positive Energy and CrossSection (required for log scale)\n    - Filters by energy range if specified\n    - Sorts by Energy to avoid line-crossing artifacts\n    \n    Args:\n        df: DataFrame with Energy and CrossSection columns\n        col_x: Name of energy column\n        col_y: Name of cross-section column\n        emin: Minimum energy (eV), optional\n        emax: Maximum energy (eV), optional\n    \n    Returns:\n        Cleaned DataFrame sorted by Energy\n    \"\"\"\n    df_clean = df.copy()\n    \n    # Drop NaN values\n    df_clean = df_clean.dropna(subset=[col_x, col_y])\n    \n    # Remove non-positive values (required for log scale)\n    df_clean = df_clean[(df_clean[col_x] > 0) & (df_clean[col_y] > 0)]\n    \n    # Apply energy range filter if specified\n    if emin is not None:\n        df_clean = df_clean[df_clean[col_x] >= emin]\n    if emax is not None:\n        df_clean = df_clean[df_clean[col_x] <= emax]\n    \n    # CRITICAL: Sort by Energy to avoid line-crossing artifacts\n    df_clean = df_clean.sort_values(by=col_x).reset_index(drop=True)\n    \n    return df_clean\n\n\ndef _clip_positive(arr, floor=1e-30):\n    \"\"\"\n    Clip array values to minimum positive floor.\n    \n    Prevents log(0) errors by ensuring all values are at least 'floor'.\n    Useful for prediction arrays that may contain zeros or negatives.\n    \n    Args:\n        arr: Numpy array or list\n        floor: Minimum positive value (default: 1e-30)\n    \n    Returns:\n        Numpy array with values >= floor\n    \"\"\"\n    arr = np.asarray(arr)\n    return np.clip(arr, floor, None)\n\n\n# ============================================================================\n# FIXED ENERGY RANGE FOR PLOTS\n# ============================================================================\nE_MIN_PLOT = 1e-4   # 10^-4 eV\nE_MAX_PLOT = 1e7    # 10^7 eV\n\n# ============================================================================\n# CREATE COMPARATIVE VISUALIZATION: U-235 (data-rich) vs Cl-35 (data-sparse)\n# ============================================================================\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7))\n\n# ============================================================================\n# LEFT PANEL: U-235 Fission (Data-Rich)\n# ============================================================================\n\nZ_u, A_u, mt_u = 92, 235, 18  # U-235 Fission\n\n# Extract U-235 fission data\nmask_u = (dataset_eval.df['Z'] == Z_u) & (dataset_eval.df['A'] == A_u) & (dataset_eval.df['MT'] == mt_u)\ndf_u = _clean_xy(dataset_eval.df[mask_u], emin=E_MIN_PLOT, emax=E_MAX_PLOT)\n\n# Get Decision Tree predictions\nenergies_dt_u, predictions_dt_u = dt_model.predict_resonance_region(\n    Z_u, A_u, mt_u, (E_MIN_PLOT, E_MAX_PLOT), num_points=500\n)\npredictions_dt_u = _clip_positive(predictions_dt_u)\n\n# Plot ground truth (blue scatter, behind)\nax1.scatter(df_u['Energy'], df_u['CrossSection'], \n            s=30, c='tab:blue', marker='o', \n            label=f'Ground Truth ({len(df_u)} EXFOR pts)', \n            alpha=0.5, zorder=1, edgecolors='none')\n\n# Plot Decision Tree prediction (red line, on top)\nax1.plot(energies_dt_u, predictions_dt_u, \n         'tab:red', linewidth=2.5, \n         label='Decision Tree (Staircase)', \n         alpha=0.9, zorder=3)\n\n# Configure axes\nax1.set_xscale('log')\nax1.set_yscale('log')\nax1.set_xlim(E_MIN_PLOT, E_MAX_PLOT)\nax1.set_xlabel('Energy (eV)', fontsize=12, fontweight='bold')\nax1.set_ylabel('Cross Section (barns)', fontsize=12, fontweight='bold')\nax1.set_title('U-235 Fission (DATA-RICH): Staircase Effect\\n' + \n              f'{len(df_u)} EXFOR measurements in range\\n' +\n              '(Model trained on full EXFOR database)',\n              fontsize=13, fontweight='bold')\nax1.legend(fontsize=11, loc='best')\nax1.grid(True, alpha=0.3, which='both')\n\n# Annotate the problem\nif len(predictions_dt_u) > 250:\n    mid_idx = 250\n    ax1.annotate('Unphysical steps!\\n(Real resonances are smooth)',\n                 xy=(energies_dt_u[mid_idx], predictions_dt_u[mid_idx]), \n                 xytext=(energies_dt_u[mid_idx] * 2, predictions_dt_u[mid_idx] * 3),\n                 arrowprops=dict(arrowstyle='->', color='tab:red', lw=2),\n                 fontsize=10, color='tab:red', fontweight='bold',\n                 bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.7))\n\n# ============================================================================\n# RIGHT PANEL: Cl-35 (n,p) (Data-Sparse)\n# ============================================================================\n\nZ_cl, A_cl, mt_cl = 17, 35, 103  # Cl-35 (n,p)\n\n# Extract Cl-35 (n,p) data\nmask_cl = (dataset_eval.df['Z'] == Z_cl) & (dataset_eval.df['A'] == A_cl) & (dataset_eval.df['MT'] == mt_cl)\ndf_cl = _clean_xy(dataset_eval.df[mask_cl], emin=E_MIN_PLOT, emax=E_MAX_PLOT)\n\n# Get Decision Tree predictions\nenergies_dt_cl, predictions_dt_cl = dt_model.predict_resonance_region(\n    Z_cl, A_cl, mt_cl, (E_MIN_PLOT, E_MAX_PLOT), num_points=500\n)\npredictions_dt_cl = _clip_positive(predictions_dt_cl)\n\n# Plot ground truth (blue scatter, behind)\nax2.scatter(df_cl['Energy'], df_cl['CrossSection'], \n            s=80, c='tab:blue', marker='o', \n            label=f'Ground Truth ({len(df_cl)} EXFOR pts)', \n            alpha=0.6, zorder=1, edgecolors='black', linewidths=1)\n\n# Plot Decision Tree prediction (red line, on top)\nax2.plot(energies_dt_cl, predictions_dt_cl, \n         'tab:red', linewidth=2.5, \n         label='Decision Tree (Extrapolation)', \n         alpha=0.9, zorder=3)\n\n# Configure axes\nax2.set_xscale('log')\nax2.set_yscale('log')\nax2.set_xlim(E_MIN_PLOT, E_MAX_PLOT)\nax2.set_xlabel('Energy (eV)', fontsize=12, fontweight='bold')\nax2.set_ylabel('Cross Section (barns)', fontsize=12, fontweight='bold')\nax2.set_title('Cl-35 (n,p) (DATA-SPARSE): Transfer Learning Test\\n' + \n              f'Only {len(df_cl)} EXFOR measurements!\\n' +\n              '(Model learned from other isotopes)',\n              fontsize=13, fontweight='bold')\nax2.legend(fontsize=11, loc='best')\nax2.grid(True, alpha=0.3, which='both')\n\n# Annotate the challenge\nif len(predictions_dt_cl) > 250:\n    mid_idx = 250\n    ax2.annotate('Can the model\\ntransfer knowledge?',\n                 xy=(energies_dt_cl[mid_idx], predictions_dt_cl[mid_idx]), \n                 xytext=(energies_dt_cl[mid_idx] * 1.5, predictions_dt_cl[mid_idx] * 0.5),\n                 arrowprops=dict(arrowstyle='->', color='tab:red', lw=2),\n                 fontsize=10, color='tab:red', fontweight='bold',\n                 bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.7))\n\nplt.tight_layout()\nplt.show()\n\n# ============================================================================\n# OBSERVATIONS\n# ============================================================================\n\nprint(\"\\n\u26a0\ufe0f  OBSERVATIONS:\")\nprint(\"=\"*80)\nprint(f\"Fixed Plot Range: {E_MIN_PLOT:.2e} to {E_MAX_PLOT:.2e} eV\")\nprint(\"Training Approach: Model trained on FULL EXFOR database (all isotopes)\")\nprint(\"Evaluation: Testing predictions on U-235 and Cl-35\")\nprint()\nprint(\"LEFT (U-235 - Data-Rich in training):\")\nprint(\"  \u2022 Decision Tree creates JAGGED predictions even with lots of training data\")\nprint(\"  \u2022 Staircase effect would cause numerical instabilities in reactor codes\")\nprint(\"  \u2022 Blue scatter: EXFOR ground truth (cleaned and sorted)\")\nprint(\"  \u2022 Red line: Decision Tree prediction showing discontinuities\")\nprint()\nprint(\"RIGHT (Cl-35 - Data-Sparse, transfer learning):\")\nprint(\"  \u2022 Model must transfer knowledge from other isotopes\")\nprint(\"  \u2022 Large gaps between measurements \u2192 Predictions test generalization\")\nprint(\"  \u2022 Blue scatter: Sparse EXFOR measurements\")\nprint(\"  \u2022 Red line: Decision Tree extrapolation\")\nprint(\"  \u2022 This is where physics-informed models REALLY shine!\")\nprint(\"=\"*80)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### \ud83d\udd34 Critical Insight #1: Piecewise Constant \u2260 Physics\n\nDecision trees partition feature space into rectangles:\n```\nif Energy < 10.5:\n    if Energy < 5.2:\n        return 150.0  # Constant!\n    else:\n        return 89.0   # Jump!\nelse:\n    return 45.0\n```\n\nReal physics:\n```\n\u03c3(E) = \u03c3_0 * \u0393 / ((E - E_r)\u00b2 + \u0393\u00b2/4)  # Smooth Breit-Wigner!\n```\n\n**Important**: The staircase effect persists even with optimal hyperparameters. This is a fundamental architectural limitation, not a tuning problem!\n\n---\n\n## Part 2: Can XGBoost Save Us?\n\nLet's try a more sophisticated ensemble method with optimal hyperparameters."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# HYPERPARAMETER OPTIMIZATION FOR XGBOOST (NAIVE FEATURES)\n# ============================================================================\n\nprint(\"=\"*80)\nprint(\"STEP 1: HYPERPARAMETER OPTIMIZATION - XGBoost (Naive Features)\")\nprint(\"=\"*80)\nprint(\"Finding optimal XGBoost configuration using Bayesian optimization...\")\nprint(\"This may take 10-15 minutes on large datasets.\\n\")\n\n# Initialize temporary model for optimization\nxgb_optimizer_naive = XGBoostEvaluator()\n\n# Run hyperparameter optimization\nopt_result_xgb_naive = xgb_optimizer_naive.optimize_hyperparameters(\n    df_tier,\n    max_evals=50,          # Increase to 100+ for production\n    cv_folds=3,\n    verbose=True\n)\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"STEP 2: TRAINING FINAL MODEL WITH OPTIMAL HYPERPARAMETERS\")\nprint(\"=\"*80)\n\n# Create model with optimal hyperparameters\nxgb_naive = XGBoostEvaluator(**opt_result_xgb_naive['best_params'])\n\n# Train on full dataset\nxgb_metrics_naive = xgb_naive.train(df_tier)\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Optimized XGBoost Performance (Naive Features):\")\nprint(\"=\"*60)\nfor key, value in xgb_metrics_naive.items():\n    if value is not None:\n        print(f\"  {key:20s}: {value}\")\n\nprint(\"\\n\ud83d\udca1 Note: These are OPTIMAL hyperparameters found via Bayesian optimization.\")\nprint(\"   Any remaining issues are architectural, not due to poor tuning!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# FIXED ENERGY RANGE FOR COMPARISON PLOTS\n# ============================================================================\nE_MIN_PLOT = 1e-4   # 10^-4 eV\nE_MAX_PLOT = 1e7    # 10^7 eV\n\n# U-235 fission comparison\nZ, A, mt_code = 92, 235, 18\nenergy_range = (E_MIN_PLOT, E_MAX_PLOT)\n\n# Get XGBoost predictions\nenergies_xgb, predictions_xgb = xgb_naive.predict_resonance_region(\n    Z, A, mt_code, energy_range, num_points=1000\n)\n\n# Get ground truth from evaluation dataset\nmask = (dataset_eval.df['Z'] == Z) & (dataset_eval.df['A'] == A) & (dataset_eval.df['MT'] == mt_code)\ndf_truth = dataset_eval.df[mask].copy()\ndf_truth = df_truth[(df_truth['Energy'] >= energy_range[0]) & (df_truth['Energy'] <= energy_range[1])]\n\n# Get Decision Tree predictions\nenergies_dt, predictions_dt = dt_model.predict_resonance_region(\n    Z, A, mt_code, energy_range, num_points=1000\n)\n\n# Comparative plot\nfig, ax = plt.subplots(figsize=(12, 6))\n\n# Ground truth - BEHIND (zorder=1)\nax.plot(df_truth['Energy'], df_truth['CrossSection'], \n        'b-', linewidth=2.5, label='Ground Truth (EXFOR)', alpha=0.6, zorder=1)\n\n# Decision Tree (stairs) - MIDDLE (zorder=2)\nax.plot(energies_dt, predictions_dt, \n        'r--', linewidth=2.0, label='Decision Tree (Staircase)', alpha=0.8, zorder=2)\n\n# XGBoost (smoother but not smooth) - ON TOP (zorder=3)\nax.plot(energies_xgb, predictions_xgb, \n        'g-', linewidth=2.5, label='XGBoost (Better, but...)', alpha=0.9, zorder=3)\n\nax.set_xscale('log')\nax.set_yscale('log')\nax.set_xlim(E_MIN_PLOT, E_MAX_PLOT)\nax.set_xlabel('Energy (eV)', fontsize=12, fontweight='bold')\nax.set_ylabel('Cross Section (barns)', fontsize=12, fontweight='bold')\nax.set_title('XGBoost vs Decision Tree: Improvement but Still Not Physics-Compliant\\nU-235 Fission (Model trained on full EXFOR)',\n             fontsize=14, fontweight='bold')\nax.legend(fontsize=11)\nax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n\u2713 XGBoost is SMOOTHER (ensemble averaging)\")\nprint(\"\u2717 But still has micro-steps and can't guarantee smoothness\")\nprint(\"\u2717 No awareness of resonance physics\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \ud83d\udfe1 Critical Insight #2: Ensembles Help, But...\n",
    "\n",
    "XGBoost averages many trees, which smooths predictions.\n",
    "\n",
    "**BUT:**\n",
    "- Still piecewise constant at fine scale\n",
    "- No guarantee of smoothness\n",
    "- Can't learn resonance physics (Breit-Wigner shape)\n",
    "- Poor extrapolation beyond training data\n",
    "\n",
    "---\n",
    "\n",
    "## Part 3: The Upgrade - Physics-Aware Features\n",
    "\n",
    "What if we give XGBoost *better features*?\n",
    "\n",
    "Instead of naive [Z, A, E, MT_onehot], use physics-derived features from the graph:\n",
    "- **Q-value**: Reaction energy\n",
    "- **Threshold**: E_threshold\n",
    "- **\u0394Z, \u0394A**: Nuclear topology\n",
    "\n",
    "This is the bridge to deep learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Get physics-aware tabular projection from FULL training dataset\ndf_physics = dataset_full.to_tabular(mode='physics')\n\nprint(\"Physics-Aware Features (trained on full EXFOR):\")\nprint(df_physics.columns.tolist())\nprint(f\"\\nDataset shape: {df_physics.shape}\")\nprint(f\"\\nFirst few rows:\")\ndf_physics.head()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# HYPERPARAMETER OPTIMIZATION FOR XGBOOST (PHYSICS FEATURES)\n# ============================================================================\n\nprint(\"=\"*80)\nprint(\"STEP 1: HYPERPARAMETER OPTIMIZATION - XGBoost (Physics Features)\")\nprint(\"=\"*80)\nprint(\"Finding optimal XGBoost configuration for physics-aware features...\")\nprint(\"This may take 10-15 minutes on large datasets.\\n\")\n\n# Initialize temporary model for optimization\nxgb_optimizer_physics = XGBoostEvaluator()\n\n# Run hyperparameter optimization\nopt_result_xgb_physics = xgb_optimizer_physics.optimize_hyperparameters(\n    df_physics,\n    max_evals=50,          # Increase to 100+ for production\n    cv_folds=3,\n    verbose=True\n)\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"STEP 2: TRAINING FINAL MODEL WITH OPTIMAL HYPERPARAMETERS\")\nprint(\"=\"*80)\n\n# Create model with optimal hyperparameters\nxgb_physics = XGBoostEvaluator(**opt_result_xgb_physics['best_params'])\n\n# Train on full dataset\nxgb_metrics_physics = xgb_physics.train(df_physics)\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Optimized XGBoost Performance (Physics Features):\")\nprint(\"=\"*60)\nfor key, value in xgb_metrics_physics.items():\n    if value is not None:\n        print(f\"  {key:20s}: {value}\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"COMPARISON: Naive vs Physics Features (both optimized)\")\nprint(\"=\"*60)\nprint(f\"  Test MSE (Naive):   {xgb_metrics_naive['test_mse']:.4e}\")\nprint(f\"  Test MSE (Physics): {xgb_metrics_physics['test_mse']:.4e}\")\nimprovement = (xgb_metrics_naive['test_mse'] - xgb_metrics_physics['test_mse']) / xgb_metrics_naive['test_mse'] * 100\nprint(f\"  Improvement: {improvement:.1f}%\")\nprint(\"\\n\ud83d\udca1 Physics features provide measurable improvement, but fundamental issues remain!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# FIXED ENERGY RANGE FOR FINAL COMPARISON\n# ============================================================================\nE_MIN_PLOT = 1e-4   # 10^-4 eV\nE_MAX_PLOT = 1e7    # 10^7 eV\n\nenergy_range = (E_MIN_PLOT, E_MAX_PLOT)\n\n# Get physics-mode predictions for U-235\nenergies_xgb_phys, predictions_xgb_phys = xgb_physics.predict_resonance_region(\n    Z, A, mt_code, energy_range, num_points=1000, mode='physics'\n)\n\n# Final comparison plot\nfig, ax = plt.subplots(figsize=(14, 7))\n\n# Ground truth - BEHIND (zorder=1)\nax.plot(df_truth['Energy'], df_truth['CrossSection'], \n        'b-', linewidth=2.5, label='Ground Truth (EXFOR)', alpha=0.6, zorder=1)\n\n# XGBoost naive - MIDDLE (zorder=2)\nax.plot(energies_xgb, predictions_xgb, \n        'orange', linewidth=2.0, linestyle='--', label='XGBoost (Naive Features)', alpha=0.7, zorder=2)\n\n# XGBoost physics - ON TOP (zorder=3)\nax.plot(energies_xgb_phys, predictions_xgb_phys, \n        'g-', linewidth=2.5, label='XGBoost (Physics Features)', alpha=0.9, zorder=3)\n\nax.set_xscale('log')\nax.set_yscale('log')\nax.set_xlim(E_MIN_PLOT, E_MAX_PLOT)\nax.set_xlabel('Energy (eV)', fontsize=13, fontweight='bold')\nax.set_ylabel('Cross Section (barns)', fontsize=13, fontweight='bold')\nax.set_title('Physics Features Help... But We Can Do Better!\\nU-235 Fission Resonance Region (Model trained on full EXFOR)',\n             fontsize=15, fontweight='bold')\nax.legend(fontsize=12, loc='best')\nax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n\u2713 Physics features improve accuracy\")\nprint(\"\u2713 Model learns about thresholds and reaction energetics\")\nprint(\"\u2713 Training on full EXFOR allows transfer learning to specific isotopes\")\nprint(\"\u2717 STILL can't guarantee smooth resonance curves\")\nprint(\"\u2717 STILL poor extrapolation to unseen energy ranges\")\nprint(\"\u2717 No explicit physics constraints (unitarity, conservation laws)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \ud83d\udfe2 Critical Insight #3: Features Matter, But Architecture Matters More\n",
    "\n",
    "Physics-aware features help XGBoost understand reactions better.\n",
    "\n",
    "**BUT** the fundamental problem remains:\n",
    "- Tree-based models are **piecewise constant**\n",
    "- No inductive bias for **smoothness**\n",
    "- No way to encode **physical constraints**\n",
    "\n",
    "---\n",
    "\n",
    "## Part 4: Feature Importance Analysis\n",
    "\n",
    "Let's see what XGBoost \"thinks\" is important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance\n",
    "importance_physics = xgb_physics.get_feature_importance()\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.barh(importance_physics['Feature'], importance_physics['Importance'])\n",
    "ax.set_xlabel('Importance (Gain)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('XGBoost Feature Importance (Physics Mode)', fontsize=14, fontweight='bold')\n",
    "ax.invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 5 Most Important Features:\")\n",
    "print(importance_physics.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### \ud83c\udf93 Key Takeaway\n\n> **Low MSE on test data does NOT guarantee safe reactor predictions!**\n>\n> We need models that:\n> 1. Respect physics (smoothness, thresholds, unitarity)\n> 2. Extrapolate correctly (beyond training data)\n> 3. Prioritize safety-critical reactions (sensitivity weighting)\n> 4. **Handle data-sparse scenarios** (like Cl-35) without overfitting\n>\n> This is why we need **Physics-Informed Deep Learning**.\n\n---\n\n## Summary: Why Classical ML Fails\n\n| Issue | U-235 (Data-Rich) | Cl-35 (Data-Sparse) |\n|-------|-------------------|---------------------|\n| Staircase Effect | \ud83d\udd34 Severe (even with lots of data) | \ud83d\udd34 Severe |\n| Interpolation | \ud83d\udfe1 Approximate | \ud83d\udd34 Very poor (large gaps) |\n| Extrapolation | \ud83d\udd34 Fails | \ud83d\udd34 Completely fails |\n| Physics Constraints | \ud83d\udd34 None | \ud83d\udd34 None |\n| Uncertainty Quantification | \ud83d\udd34 Poor | \ud83d\udd34 Very poor |\n| Training Speed | \ud83d\udfe2 Fast | \ud83d\udfe2 Fast |\n\n### The Path Forward\n\nWe need:\n1. **Graph Neural Networks** \u2192 Learn nuclear topology (not just Z, A)\n2. **Transformers** \u2192 Learn smooth energy sequences \u03c3(E)\n3. **Physics-Informed Loss** \u2192 Enforce unitarity, thresholds, conservation\n4. **Transfer Learning** \u2192 Use U-235 knowledge to improve Cl-35 predictions\n5. **Uncertainty Quantification** \u2192 Know when to trust sparse-data predictions\n\n---\n\n## Next Steps\n\nIn **Notebook 01**, we'll:\n- Build the **Chart of Nuclides as a Graph**\n- Visualize nuclear topology connecting U-235 and Cl-35\n- Understand how GNNs can transfer knowledge between isotopes\n\nIn **Notebook 02**, we'll:\n- Implement **GNN + Transformer**\n- Train on graph-structured real data\n- See **smooth, physics-compliant predictions** for both isotopes!\n\nIn **Notebook 03**, we'll:\n- Integrate with **OpenMC** for U-235 reactor validation\n- Achieve reactor-grade accuracy with real nuclear data\n- Demonstrate uncertainty quantification for Cl-35\n\nContinue to `01_Data_Fabric_and_Graph.ipynb` \u2192"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}