{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Notebook 00: Baselines and Limitations\n\n## Understanding Why Classical ML Fails for Nuclear Data\n\n**Learning Objective:** Understand *why* classical machine learning fails for nuclear data evaluation using real experimental data.\n\n**Focus Isotopes:**\n- **U-235 Fission** (data-rich, well-understood): Critical for nuclear reactors\n- **Cl-35 (n,p)** (data-sparse, research interest): Important for astrophysics and medical applications\n\n### The Problem\n\nNuclear cross sections œÉ(E) are smooth, continuous functions of energy. They exhibit:\n- **Resonance peaks**: Sharp but smooth features (especially visible in U-235)\n- **Threshold behavior**: œÉ(E) = 0 for E < E_threshold, then rises smoothly\n- **Physical constraints**: Conservation laws, unitarity, causality\n\n### Why This Matters\n\nA reactor calculation uses millions of cross-section evaluations. If predictions are:\n- **Jagged** ‚Üí Unphysical neutron transport\n- **Discontinuous** ‚Üí Numerical instabilities\n- **Wrong at key energies** ‚Üí Incorrect k_eff (criticality)\n\nThis is the **Validation Paradox**: Low MSE ‚â† Safe Reactor!\n\n**Additional Challenge:** How do models perform when data is sparse (like Cl-35)?\n\n---\n\n## Part 1: The Naive Approach\n\nLet's examine why tree-based models struggle with real nuclear cross-section data from IAEA EXFOR."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nsys.path.append('..')\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pathlib import Path\n\nfrom nucml_next.data import NucmlDataset\nfrom nucml_next.baselines import XGBoostEvaluator, DecisionTreeEvaluator\n\n# Set plotting style\nplt.style.use('seaborn-v0_8-darkgrid')\nsns.set_palette(\"husl\")\n%matplotlib inline\n\n# Verify EXFOR data exists\nexfor_path = Path('../data/exfor_processed.parquet')\nif not exfor_path.exists():\n    raise FileNotFoundError(\n        f\"EXFOR data not found at {exfor_path}\\n\"\n        \"Please run: python scripts/ingest_exfor.py --exfor-root <path> --output data/exfor_processed.parquet\"\n    )\n\nprint(\"‚úì Imports successful\")\nprint(\"‚úì EXFOR data found\")\nprint(\"Welcome to NUCML-Next: Understanding ML Limitations with Real Nuclear Data\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Step 1.1: Load Real EXFOR Data (Tabular View)\n\nWe'll train on the **full EXFOR database** (all isotopes), then evaluate on U-235 and Cl-35. This is the correct ML approach:\n- **Training**: Learn general nuclear physics patterns from ALL available data\n- **Evaluation**: Test predictions on specific target isotopes (U-235, Cl-35)\n\nThis demonstrates true transfer learning and generalization, not just memorization!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# CRITICAL FIX: Train on ENTIRE EXFOR database, not just U-235 and Cl-35!\n# Otherwise we're just memorizing the test set (overfitting)\n\n# Load FULL dataset for training (NO isotope filters)\nprint(\"Loading ENTIRE EXFOR database for training...\")\ndataset_full = NucmlDataset(\n    data_path='../data/exfor_processed.parquet',\n    mode='tabular',\n    # No Z/A filters = train on all available isotopes\n    filters={\n        'MT': [18, 102, 103]  # Focus on fission, capture, (n,p) reactions\n    }\n)\n\n# Project to tabular format with NAIVE features\ndf_naive = dataset_full.to_tabular(mode='naive')\n\nprint(f\"‚úì Full training dataset: {df_naive.shape}\")\nprint(f\"\\nFeatures (Naive Mode):\")\nprint(df_naive.columns.tolist())\n\n# Show isotope distribution in training data\nprint(\"\\nüìä Training Data Distribution (Top 10 Isotopes):\")\nisotope_counts = dataset_full.df.groupby(['Z', 'A']).size().sort_values(ascending=False).head(10)\nfor (z, a), count in isotope_counts.items():\n    # Simple element lookup (extend as needed)\n    element_map = {92: 'U', 17: 'Cl', 94: 'Pu', 26: 'Fe', 8: 'O', 1: 'H',\n                   82: 'Pb', 6: 'C', 13: 'Al', 7: 'N', 11: 'Na', 79: 'Au'}\n    elem = element_map.get(z, f'Z{z}')\n    print(f\"  {elem}-{a:3d}: {count:>8,} measurements\")\n\nprint(f\"\\n‚úì Total isotopes in training: {len(isotope_counts)} unique Z/A combinations\")\nprint(f\"‚úì This allows the model to learn general nuclear physics patterns!\")\n\n# Now load our evaluation targets (U-235 and Cl-35)\nprint(\"\\nLoading evaluation targets (U-235 and Cl-35)...\")\ndataset_eval = NucmlDataset(\n    data_path='../data/exfor_processed.parquet',\n    mode='tabular',\n    filters={\n        'Z': [92, 17],     # Uranium and Chlorine\n        'A': [235, 35],    # U-235 and Cl-35\n        'MT': [18, 102, 103]\n    }\n)\n\nprint(f\"‚úì Evaluation dataset: {len(dataset_eval.df)} measurements\")\nprint(\"\\nüìä Evaluation Isotopes:\")\nfor (z, a), group in dataset_eval.df.groupby(['Z', 'A']):\n    isotope = f\"{'U' if z==92 else 'Cl'}-{a}\"\n    print(f\"  {isotope:8s}: {len(group):>6,} measurements\")\n\ndf_naive.head()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notice:** The naive approach treats reactions as independent categories (MT_2, MT_18, etc.).\n",
    "\n",
    "**Problem:** This ignores physics! (n,2n) and (n,3n) are related - they differ by one neutron.\n",
    "\n",
    "But tree-based models don't know this. To them, MT=16 and MT=17 are just labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.2: Train Decision Tree (The \"Villain\")\n",
    "\n",
    "We'll intentionally configure the tree to show the **staircase effect**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Decision Tree with limited depth (exaggerates stairs)\n",
    "dt_model = DecisionTreeEvaluator(\n",
    "    max_depth=6,          # Shallow tree = coarse stairs\n",
    "    min_samples_leaf=20,  # Large leaves = big steps\n",
    ")\n",
    "\n",
    "# Train on naive features\n",
    "dt_metrics = dt_model.train(df_naive)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Decision Tree Performance:\")\n",
    "print(\"=\"*60)\n",
    "for key, value in dt_metrics.items():\n",
    "    print(f\"  {key:20s}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Step 1.3: The Failure Mode - Visualize the Staircase Effect\n\nLet's predict cross sections for both isotopes and see what happens...\n\n**U-235**: Rich data ‚Üí Can the model capture resonances?  \n**Cl-35**: Sparse data ‚Üí Can the model interpolate gaps?"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create comparative visualization: U-235 (data-rich) vs Cl-35 (data-sparse)\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7))\n\n# LEFT: U-235 fission in resonance region\nZ_u, A_u = 92, 235\nmt_u = 18  # Fission\nenergy_range_u = (1.0, 100.0)  # 1-100 eV (resonance region)\n\n# Get U-235 ground truth from evaluation dataset\nmask_u = (dataset_eval.df['Z'] == Z_u) & (dataset_eval.df['A'] == A_u) & (dataset_eval.df['MT'] == mt_u)\ndf_truth_u = dataset_eval.df[mask_u].copy()\ndf_truth_u = df_truth_u[(df_truth_u['Energy'] >= energy_range_u[0]) & \n                         (df_truth_u['Energy'] <= energy_range_u[1])]\n\nif len(df_truth_u) > 0:\n    # Get Decision Tree predictions (dense sampling to see steps)\n    energies_dt_u, predictions_dt_u = dt_model.predict_resonance_region(\n        Z_u, A_u, mt_u, energy_range_u, num_points=500, mode='naive'\n    )\n    \n    # Plot U-235\n    ax1.plot(df_truth_u['Energy'], df_truth_u['CrossSection'], \n            'b-', linewidth=2.5, label='Ground Truth (EXFOR)', alpha=0.7, zorder=1)\n    ax1.plot(energies_dt_u, predictions_dt_u, \n            'r-', linewidth=1.5, label='Decision Tree (Staircase)', alpha=0.8, zorder=2)\n    \n    ax1.set_xlabel('Energy (eV)', fontsize=12, fontweight='bold')\n    ax1.set_ylabel('Cross Section (barns)', fontsize=12, fontweight='bold')\n    ax1.set_title('U-235 Fission (DATA-RICH): Staircase Effect\\n' + \n                  f'{len(df_truth_u)} EXFOR measurements in range\\n' +\n                  f'(Model trained on full EXFOR database)',\n                  fontsize=13, fontweight='bold')\n    ax1.legend(fontsize=11)\n    ax1.set_yscale('log')\n    ax1.grid(True, alpha=0.3)\n    \n    # Annotate the problem\n    ax1.annotate('Unphysical steps!\\n(Real resonances are smooth)',\n                xy=(30, predictions_dt_u[150]), xytext=(50, predictions_dt_u[150]*3),\n                arrowprops=dict(arrowstyle='->', color='red', lw=2),\n                fontsize=10, color='red', fontweight='bold',\n                bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.7))\nelse:\n    ax1.text(0.5, 0.5, 'No U-235 fission data in range\\n(Check EXFOR ingestion)',\n             ha='center', va='center', transform=ax1.transAxes, fontsize=11)\n    ax1.set_title('U-235 Fission (No Data)', fontsize=13, fontweight='bold')\n\n# RIGHT: Cl-35 (n,p) reaction\nZ_cl, A_cl = 17, 35\nmt_cl = 103  # (n,p)\nenergy_range_cl = (1e6, 2e7)  # 1-20 MeV (fast neutron region)\n\n# Get Cl-35 ground truth from evaluation dataset\nmask_cl = (dataset_eval.df['Z'] == Z_cl) & (dataset_eval.df['A'] == A_cl) & (dataset_eval.df['MT'] == mt_cl)\ndf_truth_cl = dataset_eval.df[mask_cl].copy()\ndf_truth_cl = df_truth_cl[(df_truth_cl['Energy'] >= energy_range_cl[0]) & \n                           (df_truth_cl['Energy'] <= energy_range_cl[1])]\n\nif len(df_truth_cl) > 0:\n    # Get Decision Tree predictions\n    energies_dt_cl, predictions_dt_cl = dt_model.predict_resonance_region(\n        Z_cl, A_cl, mt_cl, energy_range_cl, num_points=500, mode='naive'\n    )\n    \n    # Plot Cl-35\n    ax2.scatter(df_truth_cl['Energy'], df_truth_cl['CrossSection'], \n               s=80, c='blue', marker='o', label=f'Ground Truth ({len(df_truth_cl)} EXFOR pts)', \n               alpha=0.7, zorder=2, edgecolors='black', linewidths=1)\n    ax2.plot(energies_dt_cl, predictions_dt_cl, \n            'r-', linewidth=1.5, label='Decision Tree (Extrapolation)', alpha=0.8, zorder=1)\n    \n    ax2.set_xlabel('Energy (eV)', fontsize=12, fontweight='bold')\n    ax2.set_ylabel('Cross Section (barns)', fontsize=12, fontweight='bold')\n    ax2.set_title('Cl-35 (n,p) (DATA-SPARSE): Transfer Learning Test\\n' + \n                  f'Only {len(df_truth_cl)} EXFOR measurements!\\n' +\n                  f'(Model learned from other isotopes)',\n                  fontsize=13, fontweight='bold')\n    ax2.legend(fontsize=11)\n    ax2.set_xscale('log')\n    ax2.grid(True, alpha=0.3)\n    \n    # Annotate the challenge\n    ax2.annotate('Can the model\\ntransfer knowledge?',\n                xy=(energies_dt_cl[250], predictions_dt_cl[250]), \n                xytext=(energies_dt_cl[350], predictions_dt_cl[250]*0.5),\n                arrowprops=dict(arrowstyle='->', color='red', lw=2),\n                fontsize=10, color='red', fontweight='bold',\n                bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.7))\nelse:\n    ax2.text(0.5, 0.5, 'No Cl-35 (n,p) data in range\\n(Check EXFOR ingestion or expand --max-files)',\n             ha='center', va='center', transform=ax2.transAxes, fontsize=11)\n    ax2.set_title('Cl-35 (n,p) (No Data)', fontsize=13, fontweight='bold')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n‚ö†Ô∏è  OBSERVATIONS:\")\nprint(\"=\"*80)\nprint(\"Training Approach: Model trained on FULL EXFOR database (all isotopes)\")\nprint(\"Evaluation: Testing predictions on U-235 and Cl-35\")\nprint()\nprint(\"LEFT (U-235 - Data-Rich in training):\")\nprint(\"  ‚Ä¢ Decision Tree creates JAGGED predictions even with lots of training data\")\nprint(\"  ‚Ä¢ Staircase effect would cause numerical instabilities in reactor codes\")\nprint()\nprint(\"RIGHT (Cl-35 - Data-Sparse, transfer learning):\")\nprint(\"  ‚Ä¢ Model must transfer knowledge from other isotopes\")\nprint(\"  ‚Ä¢ Large gaps between measurements ‚Üí Predictions test generalization\")\nprint(\"  ‚Ä¢ This is where physics-informed models REALLY shine!\")\nprint(\"=\"*80)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üî¥ Critical Insight #1: Piecewise Constant ‚â† Physics\n",
    "\n",
    "Decision trees partition feature space into rectangles:\n",
    "```\n",
    "if Energy < 10.5:\n",
    "    if Energy < 5.2:\n",
    "        return 150.0  # Constant!\n",
    "    else:\n",
    "        return 89.0   # Jump!\n",
    "else:\n",
    "    return 45.0\n",
    "```\n",
    "\n",
    "Real physics:\n",
    "```\n",
    "œÉ(E) = œÉ_0 * Œì / ((E - E_r)¬≤ + Œì¬≤/4)  # Smooth Breit-Wigner!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Part 2: Can XGBoost Save Us?\n",
    "\n",
    "Let's try a more sophisticated ensemble method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize XGBoost\n",
    "xgb_naive = XGBoostEvaluator(\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    ")\n",
    "\n",
    "# Train on naive features\n",
    "xgb_metrics_naive = xgb_naive.train(df_naive)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"XGBoost Performance (Naive Features):\")\n",
    "print(\"=\"*60)\n",
    "for key, value in xgb_metrics_naive.items():\n",
    "    if value is not None:\n",
    "        print(f\"  {key:20s}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Get XGBoost predictions for U-235 (data-rich example)\nZ, A, mt_code = 92, 235, 18  # U-235 fission\nenergy_range = (1.0, 100.0)  # Resonance region\n\nenergies_xgb, predictions_xgb = xgb_naive.predict_resonance_region(\n    Z, A, mt_code, energy_range, num_points=1000, mode='naive'\n)\n\n# Get ground truth from evaluation dataset\nmask = (dataset_eval.df['Z'] == Z) & (dataset_eval.df['A'] == A) & (dataset_eval.df['MT'] == mt_code)\ndf_truth = dataset_eval.df[mask].copy()\ndf_truth = df_truth[(df_truth['Energy'] >= energy_range[0]) & (df_truth['Energy'] <= energy_range[1])]\n\n# Get Decision Tree predictions (from earlier)\nenergies_dt, predictions_dt = dt_model.predict_resonance_region(\n    Z, A, mt_code, energy_range, num_points=1000, mode='naive'\n)\n\n# Comparative plot\nfig, ax = plt.subplots(figsize=(12, 6))\n\n# Ground truth\nax.plot(df_truth['Energy'], df_truth['CrossSection'], \n        'b-', linewidth=3, label='Ground Truth (EXFOR)', alpha=0.7, zorder=1)\n\n# Decision Tree (stairs)\nax.plot(energies_dt, predictions_dt, \n        'r--', linewidth=1.5, label='Decision Tree (Staircase)', alpha=0.6, zorder=2)\n\n# XGBoost (smoother but not smooth)\nax.plot(energies_xgb, predictions_xgb, \n        'g-', linewidth=2, label='XGBoost (Better, but...)', alpha=0.8, zorder=3)\n\nax.set_xlabel('Energy (eV)', fontsize=12, fontweight='bold')\nax.set_ylabel('Cross Section (barns)', fontsize=12, fontweight='bold')\nax.set_title('XGBoost vs Decision Tree: Improvement but Still Not Physics-Compliant\\nU-235 Fission (Model trained on full EXFOR)',\n             fontsize=14, fontweight='bold')\nax.legend(fontsize=11)\nax.set_yscale('log')\nax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n‚úì XGBoost is SMOOTHER (ensemble averaging)\")\nprint(\"‚úó But still has micro-steps and can't guarantee smoothness\")\nprint(\"‚úó No awareness of resonance physics\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üü° Critical Insight #2: Ensembles Help, But...\n",
    "\n",
    "XGBoost averages many trees, which smooths predictions.\n",
    "\n",
    "**BUT:**\n",
    "- Still piecewise constant at fine scale\n",
    "- No guarantee of smoothness\n",
    "- Can't learn resonance physics (Breit-Wigner shape)\n",
    "- Poor extrapolation beyond training data\n",
    "\n",
    "---\n",
    "\n",
    "## Part 3: The Upgrade - Physics-Aware Features\n",
    "\n",
    "What if we give XGBoost *better features*?\n",
    "\n",
    "Instead of naive [Z, A, E, MT_onehot], use physics-derived features from the graph:\n",
    "- **Q-value**: Reaction energy\n",
    "- **Threshold**: E_threshold\n",
    "- **ŒîZ, ŒîA**: Nuclear topology\n",
    "\n",
    "This is the bridge to deep learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Get physics-aware tabular projection from FULL training dataset\ndf_physics = dataset_full.to_tabular(mode='physics')\n\nprint(\"Physics-Aware Features (trained on full EXFOR):\")\nprint(df_physics.columns.tolist())\nprint(f\"\\nDataset shape: {df_physics.shape}\")\nprint(f\"\\nFirst few rows:\")\ndf_physics.head()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train XGBoost with physics features\n",
    "xgb_physics = XGBoostEvaluator(\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    ")\n",
    "\n",
    "xgb_metrics_physics = xgb_physics.train(df_physics)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"XGBoost Performance (Physics Features):\")\n",
    "print(\"=\"*60)\n",
    "for key, value in xgb_metrics_physics.items():\n",
    "    if value is not None:\n",
    "        print(f\"  {key:20s}: {value}\")\n",
    "\n",
    "print(\"\\nComparison with Naive Features:\")\n",
    "print(f\"  Test MSE (Naive):   {xgb_metrics_naive['test_mse']:.4e}\")\n",
    "print(f\"  Test MSE (Physics): {xgb_metrics_physics['test_mse']:.4e}\")\n",
    "improvement = (xgb_metrics_naive['test_mse'] - xgb_metrics_physics['test_mse']) / xgb_metrics_naive['test_mse'] * 100\n",
    "print(f\"  Improvement: {improvement:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Get physics-mode predictions for U-235\nenergies_xgb_phys, predictions_xgb_phys = xgb_physics.predict_resonance_region(\n    Z, A, mt_code, energy_range, num_points=1000, mode='physics'\n)\n\n# Final comparison\nfig, ax = plt.subplots(figsize=(14, 7))\n\n# Ground truth\nax.plot(df_truth['Energy'], df_truth['CrossSection'], \n        'b-', linewidth=3, label='Ground Truth (EXFOR)', alpha=0.8, zorder=1)\n\n# XGBoost naive\nax.plot(energies_xgb, predictions_xgb, \n        'orange', linewidth=2, linestyle='--', label='XGBoost (Naive Features)', alpha=0.6, zorder=2)\n\n# XGBoost physics\nax.plot(energies_xgb_phys, predictions_xgb_phys, \n        'g-', linewidth=2.5, label='XGBoost (Physics Features)', alpha=0.8, zorder=3)\n\nax.set_xlabel('Energy (eV)', fontsize=13, fontweight='bold')\nax.set_ylabel('Cross Section (barns)', fontsize=13, fontweight='bold')\nax.set_title('Physics Features Help... But We Can Do Better!\\nU-235 Fission Resonance Region (Model trained on full EXFOR)',\n             fontsize=15, fontweight='bold')\nax.legend(fontsize=12, loc='best')\nax.set_yscale('log')\nax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n‚úì Physics features improve accuracy\")\nprint(\"‚úì Model learns about thresholds and reaction energetics\")\nprint(\"‚úì Training on full EXFOR allows transfer learning to specific isotopes\")\nprint(\"‚úó STILL can't guarantee smooth resonance curves\")\nprint(\"‚úó STILL poor extrapolation to unseen energy ranges\")\nprint(\"‚úó No explicit physics constraints (unitarity, conservation laws)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üü¢ Critical Insight #3: Features Matter, But Architecture Matters More\n",
    "\n",
    "Physics-aware features help XGBoost understand reactions better.\n",
    "\n",
    "**BUT** the fundamental problem remains:\n",
    "- Tree-based models are **piecewise constant**\n",
    "- No inductive bias for **smoothness**\n",
    "- No way to encode **physical constraints**\n",
    "\n",
    "---\n",
    "\n",
    "## Part 4: Feature Importance Analysis\n",
    "\n",
    "Let's see what XGBoost \"thinks\" is important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance\n",
    "importance_physics = xgb_physics.get_feature_importance()\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.barh(importance_physics['Feature'], importance_physics['Importance'])\n",
    "ax.set_xlabel('Importance (Gain)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('XGBoost Feature Importance (Physics Mode)', fontsize=14, fontweight='bold')\n",
    "ax.invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 5 Most Important Features:\")\n",
    "print(importance_physics.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### üéì Key Takeaway\n\n> **Low MSE on test data does NOT guarantee safe reactor predictions!**\n>\n> We need models that:\n> 1. Respect physics (smoothness, thresholds, unitarity)\n> 2. Extrapolate correctly (beyond training data)\n> 3. Prioritize safety-critical reactions (sensitivity weighting)\n> 4. **Handle data-sparse scenarios** (like Cl-35) without overfitting\n>\n> This is why we need **Physics-Informed Deep Learning**.\n\n---\n\n## Summary: Why Classical ML Fails\n\n| Issue | U-235 (Data-Rich) | Cl-35 (Data-Sparse) |\n|-------|-------------------|---------------------|\n| Staircase Effect | üî¥ Severe (even with lots of data) | üî¥ Severe |\n| Interpolation | üü° Approximate | üî¥ Very poor (large gaps) |\n| Extrapolation | üî¥ Fails | üî¥ Completely fails |\n| Physics Constraints | üî¥ None | üî¥ None |\n| Uncertainty Quantification | üî¥ Poor | üî¥ Very poor |\n| Training Speed | üü¢ Fast | üü¢ Fast |\n\n### The Path Forward\n\nWe need:\n1. **Graph Neural Networks** ‚Üí Learn nuclear topology (not just Z, A)\n2. **Transformers** ‚Üí Learn smooth energy sequences œÉ(E)\n3. **Physics-Informed Loss** ‚Üí Enforce unitarity, thresholds, conservation\n4. **Transfer Learning** ‚Üí Use U-235 knowledge to improve Cl-35 predictions\n5. **Uncertainty Quantification** ‚Üí Know when to trust sparse-data predictions\n\n---\n\n## Next Steps\n\nIn **Notebook 01**, we'll:\n- Build the **Chart of Nuclides as a Graph**\n- Visualize nuclear topology connecting U-235 and Cl-35\n- Understand how GNNs can transfer knowledge between isotopes\n\nIn **Notebook 02**, we'll:\n- Implement **GNN + Transformer**\n- Train on graph-structured real data\n- See **smooth, physics-compliant predictions** for both isotopes!\n\nIn **Notebook 03**, we'll:\n- Integrate with **OpenMC** for U-235 reactor validation\n- Achieve reactor-grade accuracy with real nuclear data\n- Demonstrate uncertainty quantification for Cl-35\n\nContinue to `01_Data_Fabric_and_Graph.ipynb` ‚Üí"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}