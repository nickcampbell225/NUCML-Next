{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Notebook 00: Baselines and Limitations\n\n## Rigorous Evaluation of Classical ML for Nuclear Data\n\n**Learning Objective:** Understand *why* classical machine learning fails for nuclear data evaluation through a rigorous, research-grade comparison using optimal hyperparameters.\n\n**Methodology:**\n- Use **Bayesian hyperparameter optimization** (hyperopt) to find optimal configurations\n- Train on **full EXFOR database** (~3.6M measurements) for maximum data availability\n- Evaluate on **data-rich** (U-235) and **data-sparse** (Cl-35) isotopes\n- Compare against physics requirements (smoothness, extrapolation, constraints)\n\n**Focus Isotopes:**\n- **U-235 Fission** (data-rich, well-understood): Critical for nuclear reactors\n- **Cl-35 (n,p)** (data-sparse, research interest): Important for astrophysics and medical applications\n\n### The Problem\n\nNuclear cross sections œÉ(E) are smooth, continuous functions of energy. They exhibit:\n- **Resonance peaks**: Sharp but smooth features (especially visible in U-235)\n- **Threshold behavior**: œÉ(E) = 0 for E < E_threshold, then rises smoothly\n- **Physical constraints**: Conservation laws, unitarity, causality\n\n### Why This Matters\n\nA reactor calculation uses millions of cross-section evaluations. If predictions are:\n- **Jagged** ‚Üí Unphysical neutron transport\n- **Discontinuous** ‚Üí Numerical instabilities\n- **Wrong at key energies** ‚Üí Incorrect k_eff (criticality)\n\nThis is the **Validation Paradox**: Low MSE ‚â† Safe Reactor!\n\n**Additional Challenge:** How do models perform when data is sparse (like Cl-35)?\n\n---\n\n## Part 1: Optimized Decision Trees\n\nWe'll use **hyperparameter optimization** to give Decision Trees their best chance. This eliminates the strawman of \"intentionally bad\" parameters and provides a fair, rigorous comparison."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nsys.path.append('..')\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pathlib import Path\n\nfrom nucml_next.data import NucmlDataset\nfrom nucml_next.baselines import XGBoostEvaluator, DecisionTreeEvaluator\n\n# Set plotting style\nplt.style.use('seaborn-v0_8-darkgrid')\nsns.set_palette(\"husl\")\n%matplotlib inline\n\n# Verify EXFOR data exists\nexfor_path = Path('../data/exfor_processed.parquet')\nif not exfor_path.exists():\n    raise FileNotFoundError(\n        f\"EXFOR data not found at {exfor_path}\\n\"\n        \"Please run: python scripts/ingest_exfor.py --exfor-root <path> --output data/exfor_processed.parquet\"\n    )\n\nprint(\"‚úì Imports successful\")\nprint(\"‚úì EXFOR data found\")\nprint(\"Welcome to NUCML-Next: Understanding ML Limitations with Real Nuclear Data\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Step 1.1: Load Real EXFOR Data (Tabular View)\n\nWe'll train on the **full EXFOR database** (all isotopes), then evaluate on U-235 and Cl-35. This is the correct ML approach:\n- **Training**: Learn general nuclear physics patterns from ALL available data\n- **Evaluation**: Test predictions on specific target isotopes (U-235, Cl-35)\n\nThis demonstrates true transfer learning and generalization, not just memorization!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# CRITICAL FIX: Use physics-aware DataSelection for scientifically defensible filtering\n# This demonstrates proper ML workflow with predicate pushdown for efficiency\n\nfrom nucml_next.data import DataSelection\n\n# Strategy: Train on neutron-induced reactions at reactor energies (scientifically defensible default)\n# This avoids training on:\n# - Bookkeeping codes (MT 0, 1, 9000+) which are arithmetic identities\n# - Non-reactor energies (too high/low for criticality calculations)\n# - Non-neutron projectiles (we're focused on reactor physics)\n\nprint(\"Creating physics-aware data selection...\")\nprint(\"=\" * 80)\n\n# Training selection: Reactor physics, neutrons, all physical reactions\ntraining_selection = DataSelection(\n    # ============================================================================\n    # PROJECTILE SELECTION\n    # ============================================================================\n    projectile='neutron',          # Options: 'neutron' | 'all'\n                                   # 'neutron' = Only neutron-induced reactions (reactor physics)\n                                   # 'all' = All projectiles (n, p, d, Œ±, Œ≥, etc.)\n    \n    # ============================================================================\n    # ENERGY RANGE (eV)\n    # ============================================================================\n    energy_min=1e-5,               # Minimum energy in eV (1e-5 = 0.01 eV, thermal neutrons)\n    energy_max=2e7,                # Maximum energy in eV (2e7 = 20 MeV, reactor physics upper bound)\n                                   # Common ranges:\n                                   #   - Thermal: 1e-5 to 1 eV\n                                   #   - Resonance: 1 to 1e4 eV\n                                   #   - Fast: 1e4 to 2e7 eV (20 MeV)\n                                   #   - High energy: up to 1e9 eV (1 GeV)\n    \n    # ============================================================================\n    # REACTION (MT) MODE SELECTION\n    # ============================================================================\n    mt_mode='all_physical',        # Options:\n                                   # 'reactor_core'   ‚Üí Essential for reactor modeling\n                                   #                    (MT 2, 4, 16, 18, 102, 103, 107)\n                                   #                    [elastic, inelastic, (n,2n), fission,\n                                   #                     capture, (n,p), (n,Œ±)]\n                                   #\n                                   # 'threshold_only' ‚Üí Reactions with energy thresholds\n                                   #                    (MT 16, 17, 103, 104, 105, 106, 107)\n                                   #                    [(n,2n), (n,3n), (n,p), (n,d), (n,t),\n                                   #                     (n,¬≥He), (n,Œ±)]\n                                   #\n                                   # 'fission_details'‚Üí Fission breakdown channels\n                                   #                    (MT 18, 19, 20, 21, 38)\n                                   #                    [total fission, 1st chance, 2nd chance,\n                                   #                     3rd chance, 4th chance]\n                                   #\n                                   # 'all_physical'   ‚Üí All codes < 9000, excluding bookkeeping\n                                   #                    (Includes all real reactions, excludes\n                                   #                     derived/total cross-sections)\n                                   #\n                                   # 'custom'         ‚Üí Use custom_mt_codes list (see below)\n    \n    custom_mt_codes=None,          # Used only when mt_mode='custom'\n                                   # Example: [2, 18, 102]  # Elastic, fission, capture\n                                   # Example: [16, 17, 18]  # (n,2n), (n,3n), fission\n                                   # Example: list(range(50, 92))  # MT 50-91 (inelastic levels)\n    \n    # ============================================================================\n    # EXCLUSION RULES\n    # ============================================================================\n    exclude_bookkeeping=True,      # Exclude MT 0, 1, and MT >= 9000\n                                   # MT 0 = Undefined\n                                   # MT 1 = Total cross-section (sum of others)\n                                   # MT >= 9000 = Lumped reaction covariances\n                                   # These are arithmetic identities, not physics!\n    \n    # ============================================================================\n    # DATA VALIDITY\n    # ============================================================================\n    drop_invalid=True,             # Drop NaN or non-positive cross-sections\n                                   # Essential for log-transform: log(œÉ) requires œÉ > 0\n                                   # Prevents training instabilities\n    \n    # ============================================================================\n    # EVALUATION CONTROLS (Holdout for Extrapolation Testing)\n    # ============================================================================\n    holdout_isotopes=None          # List of (Z, A) tuples to exclude from training\n                                   # None = Use all data (default for training)\n                                   # Example: [(92, 235)]           # Hold out U-235 only\n                                   # Example: [(92, 235), (17, 35)] # Hold out U-235 and Cl-35\n                                   # Example: [(94, 239), (94, 240), (94, 241)]  # Pu isotopes\n                                   # Use this to measure TRUE extrapolation capability!\n)\n\nprint(\"Training Selection:\")\nprint(training_selection)\nprint()\n\n# Load FULL dataset for training with physics-aware filtering\n# CRITICAL: Predicate pushdown filters at PyArrow fragment level (90% I/O reduction!)\nprint(\"=\" * 80)\nprint(\"Loading training dataset with predicate pushdown...\")\nprint(\"=\" * 80)\ndataset_full = NucmlDataset(\n    data_path='../data/exfor_processed.parquet',\n    mode='tabular',\n    selection=training_selection  # Physics-aware selection with predicate pushdown\n)\n\n# Project to tabular format with NAIVE features\nprint(\"\\n\" + \"=\" * 80)\nprint(\"Projecting to tabular format (naive mode)...\")\nprint(\"=\" * 80)\ndf_naive = dataset_full.to_tabular(mode='naive')\n\nprint(f\"\\n‚úì Full training dataset: {df_naive.shape}\")\nprint(f\"\\nFeatures (Naive Mode):\")\nprint(df_naive.columns.tolist())\n\n# Show isotope distribution in training data\nprint(\"\\nüìä Training Data Distribution (Top 10 Isotopes):\")\nisotope_counts = dataset_full.df.groupby(['Z', 'A']).size().sort_values(ascending=False).head(10)\nfor (z, a), count in isotope_counts.items():\n    # Simple element lookup (extend as needed)\n    element_map = {92: 'U', 17: 'Cl', 94: 'Pu', 26: 'Fe', 8: 'O', 1: 'H',\n                   82: 'Pb', 6: 'C', 13: 'Al', 7: 'N', 11: 'Na', 79: 'Au'}\n    elem = element_map.get(z, f'Z{z}')\n    print(f\"  {elem}-{a:3d}: {count:>8,} measurements\")\n\nprint(f\"\\n‚úì Total isotopes: {dataset_full.df.groupby(['Z', 'A']).ngroups} unique Z/A combinations\")\nprint(f\"‚úì Total reaction types: {dataset_full.df['MT'].nunique()} unique MT codes\")\nprint(f\"‚úì Total measurements: {len(dataset_full.df):,}\")\n\n# Show MT distribution\nprint(\"\\nüìä Top 10 Reaction Types (MT codes):\")\nmt_counts = dataset_full.df['MT'].value_counts().head(10)\nmt_names = {18: 'Fission', 102: '(n,Œ≥) Capture', 103: '(n,p)', 2: 'Elastic',\n            16: '(n,2n)', 17: '(n,3n)', 4: 'Inelastic', 107: '(n,Œ±)'}\nfor mt, count in mt_counts.items():\n    name = mt_names.get(mt, f'MT-{mt}')\n    print(f\"  MT {mt:3d} {name:15s}: {count:>8,} measurements\")\n\nprint(f\"\\n‚úì Training on neutron-induced reactions allows transfer learning!\")\nprint(f\"‚úì Predicate pushdown reduced load time by filtering at fragment level\")\n\n# Now load evaluation targets (U-235 and Cl-35) using legacy filters for specific selection\nprint(\"\\n\" + \"=\"*70)\nprint(\"Loading evaluation targets (U-235 and Cl-35) using legacy filters...\")\nprint(\"NOTE: For evaluation, we use legacy filters for precise isotope/MT selection\")\nprint(\"=\"*70)\ndataset_eval = NucmlDataset(\n    data_path='../data/exfor_processed.parquet',\n    mode='tabular',\n    filters={  # Legacy filters for backward compatibility\n        'Z': [92, 17],     # Uranium and Chlorine\n        'A': [235, 35],    # U-235 and Cl-35\n        'MT': [18, 102, 103]  # Fission, capture, (n,p) - for visualization\n    }\n)\n\nprint(f\"‚úì Evaluation dataset: {len(dataset_eval.df)} measurements\")\nprint(\"\\nüìä Evaluation Isotopes:\")\nfor (z, a), group in dataset_eval.df.groupby(['Z', 'A']):\n    isotope = f\"{'U' if z==92 else 'Cl'}-{a}\"\n    print(f\"  {isotope:8s}: {len(group):>6,} measurements\")\nprint(\"=\"*70)\n\ndf_naive.head()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notice:** The naive approach treats reactions as independent categories (MT_2, MT_18, etc.).\n",
    "\n",
    "**Problem:** This ignores physics! (n,2n) and (n,3n) are related - they differ by one neutron.\n",
    "\n",
    "But tree-based models don't know this. To them, MT=16 and MT=17 are just labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Step 1.2: Optimize and Train Decision Tree\n\nInstead of artificially limiting the model, we'll use **Bayesian hyperparameter optimization** to find the optimal Decision Tree configuration. This is a research-grade approach that gives classical ML its best chance to succeed.\n\n**Research Question**: Even with optimal hyperparameters, can Decision Trees produce smooth, physics-compliant cross-section predictions?"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# HYPERPARAMETER OPTIMIZATION FOR DECISION TREE\n# ============================================================================\n# Use Bayesian optimization (hyperopt) to find optimal hyperparameters\n# This ensures we're evaluating Decision Trees at their BEST, not strawman configs\n\nprint(\"=\"*80)\nprint(\"STEP 1: HYPERPARAMETER OPTIMIZATION\")\nprint(\"=\"*80)\nprint(\"Finding optimal Decision Tree configuration using Bayesian optimization...\")\nprint(\"This may take 5-10 minutes on large datasets.\\n\")\n\n# Initialize temporary model for optimization\ndt_optimizer = DecisionTreeEvaluator()\n\n# Run hyperparameter optimization (50 evaluations for speed, increase for production)\nopt_result_dt = dt_optimizer.optimize_hyperparameters(\n    df_naive,\n    max_evals=50,          # Increase to 100+ for production\n    cv_folds=3,            # 3-fold cross-validation\n    verbose=True\n)\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"STEP 2: TRAINING FINAL MODEL WITH OPTIMAL HYPERPARAMETERS\")\nprint(\"=\"*80)\n\n# Create model with optimal hyperparameters\ndt_model = DecisionTreeEvaluator(**opt_result_dt['best_params'])\n\n# Train on full dataset\ndt_metrics = dt_model.train(df_naive)\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Optimized Decision Tree Performance:\")\nprint(\"=\"*60)\nfor key, value in dt_metrics.items():\n    print(f\"  {key:20s}: {value}\")\n\nprint(\"\\nüí° Note: This is an OPTIMIZED Decision Tree, not artificially constrained.\")\nprint(\"   The staircase effect is inherent to the algorithm, not poor hyperparameters!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Step 1.3: The Failure Mode - Visualize the Staircase Effect\n\nLet's predict cross sections for both isotopes and see what happens...\n\n**U-235**: Rich data ‚Üí Can the model capture resonances?  \n**Cl-35**: Sparse data ‚Üí Can the model interpolate gaps?"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# HELPER FUNCTIONS for Clean Cross-Section Plotting\n# ============================================================================\n\ndef _clean_xy(df, col_x='Energy', col_y='CrossSection', emin=None, emax=None):\n    \"\"\"\n    Clean and prepare data for log-log plotting.\n    \n    - Drops NaN values in Energy and CrossSection\n    - Removes non-positive Energy and CrossSection (required for log scale)\n    - Filters by energy range if specified\n    - Sorts by Energy to avoid line-crossing artifacts\n    \n    Args:\n        df: DataFrame with Energy and CrossSection columns\n        col_x: Name of energy column\n        col_y: Name of cross-section column\n        emin: Minimum energy (eV), optional\n        emax: Maximum energy (eV), optional\n    \n    Returns:\n        Cleaned DataFrame sorted by Energy\n    \"\"\"\n    df_clean = df.copy()\n    \n    # Drop NaN values\n    df_clean = df_clean.dropna(subset=[col_x, col_y])\n    \n    # Remove non-positive values (required for log scale)\n    df_clean = df_clean[(df_clean[col_x] > 0) & (df_clean[col_y] > 0)]\n    \n    # Apply energy range filter if specified\n    if emin is not None:\n        df_clean = df_clean[df_clean[col_x] >= emin]\n    if emax is not None:\n        df_clean = df_clean[df_clean[col_x] <= emax]\n    \n    # CRITICAL: Sort by Energy to avoid line-crossing artifacts\n    df_clean = df_clean.sort_values(by=col_x).reset_index(drop=True)\n    \n    return df_clean\n\n\ndef _clip_positive(arr, floor=1e-30):\n    \"\"\"\n    Clip array values to minimum positive floor.\n    \n    Prevents log(0) errors by ensuring all values are at least 'floor'.\n    Useful for prediction arrays that may contain zeros or negatives.\n    \n    Args:\n        arr: Numpy array or list\n        floor: Minimum positive value (default: 1e-30)\n    \n    Returns:\n        Numpy array with values >= floor\n    \"\"\"\n    arr = np.asarray(arr)\n    return np.clip(arr, floor, None)\n\n\n# ============================================================================\n# CREATE COMPARATIVE VISUALIZATION: U-235 (data-rich) vs Cl-35 (data-sparse)\n# ============================================================================\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7))\n\n# ============================================================================\n# LEFT PANEL: U-235 Fission (Data-Rich, Resonance Region)\n# ============================================================================\n\nZ_u, A_u = 92, 235\nmt_u = 18  # Fission\nenergy_range_u = (1e-3, 1e6)  # 0.001 eV to 1 MeV (thermal to fast neutron region)\n\n# Extract and clean U-235 ground truth from evaluation dataset\nmask_u = (dataset_eval.df['Z'] == Z_u) & (dataset_eval.df['A'] == A_u) & (dataset_eval.df['MT'] == mt_u)\ndf_truth_u = dataset_eval.df[mask_u].copy()\ndf_truth_u = _clean_xy(df_truth_u, emin=energy_range_u[0], emax=energy_range_u[1])\n\nif len(df_truth_u) > 0:\n    # Get Decision Tree predictions (dense sampling to see staircase)\n    energies_dt_u, predictions_dt_u = dt_model.predict_resonance_region(\n        Z_u, A_u, mt_u, energy_range_u, num_points=500, mode='naive'\n    )\n    \n    # Clip predictions to avoid log(0) errors\n    predictions_dt_u = _clip_positive(predictions_dt_u, floor=1e-30)\n    \n    # Plot ground truth as SCATTER (blue) - BEHIND (zorder=1)\n    ax1.scatter(df_truth_u['Energy'], df_truth_u['CrossSection'], \n                s=30, c='tab:blue', marker='o', \n                label=f'Ground Truth ({len(df_truth_u)} EXFOR pts)', \n                alpha=0.5, zorder=1, edgecolors='none')\n    \n    # Plot Decision Tree prediction as LINE (red) - ON TOP (zorder=3)\n    ax1.plot(energies_dt_u, predictions_dt_u, \n             'tab:red', linewidth=2.5, \n             label='Decision Tree (Staircase)', \n             alpha=0.9, zorder=3)\n    \n    # Configure axes: LOG-LOG with specified ranges\n    ax1.set_xscale('log')\n    ax1.set_yscale('log')\n    ax1.set_xlim(1e-3, 1e6)   # Energy range: 0.001 eV to 1 MeV\n    ax1.set_ylim(1e-3, 1e5)   # Cross-section range: 0.001 to 100,000 barns\n    ax1.set_xlabel('Energy (eV)', fontsize=12, fontweight='bold')\n    ax1.set_ylabel('Cross Section (barns)', fontsize=12, fontweight='bold')\n    ax1.set_title('U-235 Fission (DATA-RICH): Staircase Effect\\n' + \n                  f'{len(df_truth_u)} EXFOR measurements in range\\n' +\n                  '(Model trained on full EXFOR database)',\n                  fontsize=13, fontweight='bold')\n    ax1.legend(fontsize=11, loc='best')\n    ax1.grid(True, alpha=0.3, which='both')  # Grid on both major and minor ticks\n    \n    # Annotate the problem - use safe indexing\n    if len(predictions_dt_u) > 250:\n        mid_idx = min(250, len(predictions_dt_u) - 1)\n        anno_x = energies_dt_u[mid_idx]\n        anno_y = predictions_dt_u[mid_idx]\n        ax1.annotate('Unphysical steps!\\n(Real resonances are smooth)',\n                     xy=(anno_x, anno_y), \n                     xytext=(anno_x * 2, anno_y * 3),\n                     arrowprops=dict(arrowstyle='->', color='tab:red', lw=2),\n                     fontsize=10, color='tab:red', fontweight='bold',\n                     bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.7))\nelse:\n    ax1.text(0.5, 0.5, 'No U-235 fission data in range\\n(Check EXFOR ingestion)',\n             ha='center', va='center', transform=ax1.transAxes, fontsize=11)\n    ax1.set_title('U-235 Fission (No Data)', fontsize=13, fontweight='bold')\n    ax1.set_xscale('log')\n    ax1.set_yscale('log')\n    ax1.set_xlim(1e-3, 1e6)\n    ax1.set_ylim(1e-3, 1e5)\n    ax1.grid(True, alpha=0.3, which='both')\n\n\n# ============================================================================\n# RIGHT PANEL: Cl-35 (n,p) (Data-Sparse, Fast Neutron Region)\n# ============================================================================\n\nZ_cl, A_cl = 17, 35\nmt_cl = 103  # (n,p)\nenergy_range_cl = (1e0, 1e7)  # 1 eV to 10 MeV (thermal to fast neutron region)\n\n# Extract and clean Cl-35 ground truth from evaluation dataset\nmask_cl = (dataset_eval.df['Z'] == Z_cl) & (dataset_eval.df['A'] == A_cl) & (dataset_eval.df['MT'] == mt_cl)\ndf_truth_cl = dataset_eval.df[mask_cl].copy()\ndf_truth_cl = _clean_xy(df_truth_cl, emin=energy_range_cl[0], emax=energy_range_cl[1])\n\nif len(df_truth_cl) > 0:\n    # Get Decision Tree predictions\n    energies_dt_cl, predictions_dt_cl = dt_model.predict_resonance_region(\n        Z_cl, A_cl, mt_cl, energy_range_cl, num_points=500, mode='naive'\n    )\n    \n    # Clip predictions to avoid log(0) errors\n    predictions_dt_cl = _clip_positive(predictions_dt_cl, floor=1e-30)\n    \n    # Plot ground truth as SCATTER (blue) - BEHIND (zorder=1)\n    ax2.scatter(df_truth_cl['Energy'], df_truth_cl['CrossSection'], \n                s=80, c='tab:blue', marker='o', \n                label=f'Ground Truth ({len(df_truth_cl)} EXFOR pts)', \n                alpha=0.6, zorder=1, edgecolors='black', linewidths=1)\n    \n    # Plot Decision Tree prediction as LINE (red) - ON TOP (zorder=3)\n    ax2.plot(energies_dt_cl, predictions_dt_cl, \n             'tab:red', linewidth=2.5, \n             label='Decision Tree (Extrapolation)', \n             alpha=0.9, zorder=3)\n    \n    # Configure axes: LOG-LOG with specified ranges\n    ax2.set_xscale('log')\n    ax2.set_yscale('log')\n    ax2.set_xlim(1e0, 1e7)    # Energy range: 1 eV to 10 MeV\n    ax2.set_ylim(1e-6, 1e1)   # Cross-section range: 1e-6 to 10 barns\n    ax2.set_xlabel('Energy (eV)', fontsize=12, fontweight='bold')\n    ax2.set_ylabel('Cross Section (barns)', fontsize=12, fontweight='bold')\n    ax2.set_title('Cl-35 (n,p) (DATA-SPARSE): Transfer Learning Test\\n' + \n                  f'Only {len(df_truth_cl)} EXFOR measurements!\\n' +\n                  '(Model learned from other isotopes)',\n                  fontsize=13, fontweight='bold')\n    ax2.legend(fontsize=11, loc='best')\n    ax2.grid(True, alpha=0.3, which='both')  # Grid on both major and minor ticks\n    \n    # Annotate the challenge - use safe indexing\n    if len(predictions_dt_cl) > 250:\n        mid_idx = min(250, len(predictions_dt_cl) - 1)\n        anno_x = energies_dt_cl[mid_idx]\n        anno_y = predictions_dt_cl[mid_idx]\n        ax2.annotate('Can the model\\ntransfer knowledge?',\n                     xy=(anno_x, anno_y), \n                     xytext=(anno_x * 1.5, anno_y * 0.5),\n                     arrowprops=dict(arrowstyle='->', color='tab:red', lw=2),\n                     fontsize=10, color='tab:red', fontweight='bold',\n                     bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.7))\nelse:\n    ax2.text(0.5, 0.5, 'No Cl-35 (n,p) data in range\\n(Check EXFOR ingestion or expand --max-files)',\n             ha='center', va='center', transform=ax2.transAxes, fontsize=11)\n    ax2.set_title('Cl-35 (n,p) (No Data)', fontsize=13, fontweight='bold')\n    ax2.set_xscale('log')\n    ax2.set_yscale('log')\n    ax2.set_xlim(1e0, 1e7)\n    ax2.set_ylim(1e-6, 1e1)\n    ax2.grid(True, alpha=0.3, which='both')\n\nplt.tight_layout()\nplt.show()\n\n# ============================================================================\n# OBSERVATIONS\n# ============================================================================\n\nprint(\"\\n‚ö†Ô∏è  OBSERVATIONS:\")\nprint(\"=\"*80)\nprint(\"Training Approach: Model trained on FULL EXFOR database (all isotopes)\")\nprint(\"Evaluation: Testing predictions on U-235 and Cl-35\")\nprint()\nprint(\"LEFT (U-235 - Data-Rich in training):\")\nprint(\"  ‚Ä¢ Decision Tree creates JAGGED predictions even with lots of training data\")\nprint(\"  ‚Ä¢ Staircase effect would cause numerical instabilities in reactor codes\")\nprint(\"  ‚Ä¢ Blue scatter: EXFOR ground truth (cleaned and sorted)\")\nprint(\"  ‚Ä¢ Red line: Decision Tree prediction showing discontinuities\")\nprint()\nprint(\"RIGHT (Cl-35 - Data-Sparse, transfer learning):\")\nprint(\"  ‚Ä¢ Model must transfer knowledge from other isotopes\")\nprint(\"  ‚Ä¢ Large gaps between measurements ‚Üí Predictions test generalization\")\nprint(\"  ‚Ä¢ Blue scatter: Sparse EXFOR measurements\")\nprint(\"  ‚Ä¢ Red line: Decision Tree extrapolation\")\nprint(\"  ‚Ä¢ This is where physics-informed models REALLY shine!\")\nprint(\"=\"*80)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### üî¥ Critical Insight #1: Piecewise Constant ‚â† Physics\n\nDecision trees partition feature space into rectangles:\n```\nif Energy < 10.5:\n    if Energy < 5.2:\n        return 150.0  # Constant!\n    else:\n        return 89.0   # Jump!\nelse:\n    return 45.0\n```\n\nReal physics:\n```\nœÉ(E) = œÉ_0 * Œì / ((E - E_r)¬≤ + Œì¬≤/4)  # Smooth Breit-Wigner!\n```\n\n**Important**: The staircase effect persists even with optimal hyperparameters. This is a fundamental architectural limitation, not a tuning problem!\n\n---\n\n## Part 2: Can XGBoost Save Us?\n\nLet's try a more sophisticated ensemble method with optimal hyperparameters."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# HYPERPARAMETER OPTIMIZATION FOR XGBOOST (NAIVE FEATURES)\n# ============================================================================\n\nprint(\"=\"*80)\nprint(\"STEP 1: HYPERPARAMETER OPTIMIZATION - XGBoost (Naive Features)\")\nprint(\"=\"*80)\nprint(\"Finding optimal XGBoost configuration using Bayesian optimization...\")\nprint(\"This may take 10-15 minutes on large datasets.\\n\")\n\n# Initialize temporary model for optimization\nxgb_optimizer_naive = XGBoostEvaluator()\n\n# Run hyperparameter optimization\nopt_result_xgb_naive = xgb_optimizer_naive.optimize_hyperparameters(\n    df_naive,\n    max_evals=50,          # Increase to 100+ for production\n    cv_folds=3,\n    verbose=True\n)\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"STEP 2: TRAINING FINAL MODEL WITH OPTIMAL HYPERPARAMETERS\")\nprint(\"=\"*80)\n\n# Create model with optimal hyperparameters\nxgb_naive = XGBoostEvaluator(**opt_result_xgb_naive['best_params'])\n\n# Train on full dataset\nxgb_metrics_naive = xgb_naive.train(df_naive)\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Optimized XGBoost Performance (Naive Features):\")\nprint(\"=\"*60)\nfor key, value in xgb_metrics_naive.items():\n    if value is not None:\n        print(f\"  {key:20s}: {value}\")\n\nprint(\"\\nüí° Note: These are OPTIMAL hyperparameters found via Bayesian optimization.\")\nprint(\"   Any remaining issues are architectural, not due to poor tuning!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Get XGBoost predictions for U-235 (data-rich example)\nZ, A, mt_code = 92, 235, 18  # U-235 fission\nenergy_range = (1.0, 100.0)  # Resonance region\n\nenergies_xgb, predictions_xgb = xgb_naive.predict_resonance_region(\n    Z, A, mt_code, energy_range, num_points=1000, mode='naive'\n)\n\n# Get ground truth from evaluation dataset\nmask = (dataset_eval.df['Z'] == Z) & (dataset_eval.df['A'] == A) & (dataset_eval.df['MT'] == mt_code)\ndf_truth = dataset_eval.df[mask].copy()\ndf_truth = df_truth[(df_truth['Energy'] >= energy_range[0]) & (df_truth['Energy'] <= energy_range[1])]\n\n# Get Decision Tree predictions (from earlier)\nenergies_dt, predictions_dt = dt_model.predict_resonance_region(\n    Z, A, mt_code, energy_range, num_points=1000, mode='naive'\n)\n\n# Comparative plot\nfig, ax = plt.subplots(figsize=(12, 6))\n\n# Ground truth - BEHIND (zorder=1)\nax.plot(df_truth['Energy'], df_truth['CrossSection'], \n        'b-', linewidth=2.5, label='Ground Truth (EXFOR)', alpha=0.6, zorder=1)\n\n# Decision Tree (stairs) - MIDDLE (zorder=2)\nax.plot(energies_dt, predictions_dt, \n        'r--', linewidth=2.0, label='Decision Tree (Staircase)', alpha=0.8, zorder=2)\n\n# XGBoost (smoother but not smooth) - ON TOP (zorder=3)\nax.plot(energies_xgb, predictions_xgb, \n        'g-', linewidth=2.5, label='XGBoost (Better, but...)', alpha=0.9, zorder=3)\n\nax.set_xlabel('Energy (eV)', fontsize=12, fontweight='bold')\nax.set_ylabel('Cross Section (barns)', fontsize=12, fontweight='bold')\nax.set_title('XGBoost vs Decision Tree: Improvement but Still Not Physics-Compliant\\nU-235 Fission (Model trained on full EXFOR)',\n             fontsize=14, fontweight='bold')\nax.legend(fontsize=11)\nax.set_yscale('log')\nax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n‚úì XGBoost is SMOOTHER (ensemble averaging)\")\nprint(\"‚úó But still has micro-steps and can't guarantee smoothness\")\nprint(\"‚úó No awareness of resonance physics\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üü° Critical Insight #2: Ensembles Help, But...\n",
    "\n",
    "XGBoost averages many trees, which smooths predictions.\n",
    "\n",
    "**BUT:**\n",
    "- Still piecewise constant at fine scale\n",
    "- No guarantee of smoothness\n",
    "- Can't learn resonance physics (Breit-Wigner shape)\n",
    "- Poor extrapolation beyond training data\n",
    "\n",
    "---\n",
    "\n",
    "## Part 3: The Upgrade - Physics-Aware Features\n",
    "\n",
    "What if we give XGBoost *better features*?\n",
    "\n",
    "Instead of naive [Z, A, E, MT_onehot], use physics-derived features from the graph:\n",
    "- **Q-value**: Reaction energy\n",
    "- **Threshold**: E_threshold\n",
    "- **ŒîZ, ŒîA**: Nuclear topology\n",
    "\n",
    "This is the bridge to deep learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Get physics-aware tabular projection from FULL training dataset\ndf_physics = dataset_full.to_tabular(mode='physics')\n\nprint(\"Physics-Aware Features (trained on full EXFOR):\")\nprint(df_physics.columns.tolist())\nprint(f\"\\nDataset shape: {df_physics.shape}\")\nprint(f\"\\nFirst few rows:\")\ndf_physics.head()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# HYPERPARAMETER OPTIMIZATION FOR XGBOOST (PHYSICS FEATURES)\n# ============================================================================\n\nprint(\"=\"*80)\nprint(\"STEP 1: HYPERPARAMETER OPTIMIZATION - XGBoost (Physics Features)\")\nprint(\"=\"*80)\nprint(\"Finding optimal XGBoost configuration for physics-aware features...\")\nprint(\"This may take 10-15 minutes on large datasets.\\n\")\n\n# Initialize temporary model for optimization\nxgb_optimizer_physics = XGBoostEvaluator()\n\n# Run hyperparameter optimization\nopt_result_xgb_physics = xgb_optimizer_physics.optimize_hyperparameters(\n    df_physics,\n    max_evals=50,          # Increase to 100+ for production\n    cv_folds=3,\n    verbose=True\n)\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"STEP 2: TRAINING FINAL MODEL WITH OPTIMAL HYPERPARAMETERS\")\nprint(\"=\"*80)\n\n# Create model with optimal hyperparameters\nxgb_physics = XGBoostEvaluator(**opt_result_xgb_physics['best_params'])\n\n# Train on full dataset\nxgb_metrics_physics = xgb_physics.train(df_physics)\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Optimized XGBoost Performance (Physics Features):\")\nprint(\"=\"*60)\nfor key, value in xgb_metrics_physics.items():\n    if value is not None:\n        print(f\"  {key:20s}: {value}\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"COMPARISON: Naive vs Physics Features (both optimized)\")\nprint(\"=\"*60)\nprint(f\"  Test MSE (Naive):   {xgb_metrics_naive['test_mse']:.4e}\")\nprint(f\"  Test MSE (Physics): {xgb_metrics_physics['test_mse']:.4e}\")\nimprovement = (xgb_metrics_naive['test_mse'] - xgb_metrics_physics['test_mse']) / xgb_metrics_naive['test_mse'] * 100\nprint(f\"  Improvement: {improvement:.1f}%\")\nprint(\"\\nüí° Physics features provide measurable improvement, but fundamental issues remain!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Get physics-mode predictions for U-235\nenergies_xgb_phys, predictions_xgb_phys = xgb_physics.predict_resonance_region(\n    Z, A, mt_code, energy_range, num_points=1000, mode='physics'\n)\n\n# Final comparison\nfig, ax = plt.subplots(figsize=(14, 7))\n\n# Ground truth - BEHIND (zorder=1)\nax.plot(df_truth['Energy'], df_truth['CrossSection'], \n        'b-', linewidth=2.5, label='Ground Truth (EXFOR)', alpha=0.6, zorder=1)\n\n# XGBoost naive - MIDDLE (zorder=2)\nax.plot(energies_xgb, predictions_xgb, \n        'orange', linewidth=2.0, linestyle='--', label='XGBoost (Naive Features)', alpha=0.7, zorder=2)\n\n# XGBoost physics - ON TOP (zorder=3)\nax.plot(energies_xgb_phys, predictions_xgb_phys, \n        'g-', linewidth=2.5, label='XGBoost (Physics Features)', alpha=0.9, zorder=3)\n\nax.set_xlabel('Energy (eV)', fontsize=13, fontweight='bold')\nax.set_ylabel('Cross Section (barns)', fontsize=13, fontweight='bold')\nax.set_title('Physics Features Help... But We Can Do Better!\\nU-235 Fission Resonance Region (Model trained on full EXFOR)',\n             fontsize=15, fontweight='bold')\nax.legend(fontsize=12, loc='best')\nax.set_yscale('log')\nax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n‚úì Physics features improve accuracy\")\nprint(\"‚úì Model learns about thresholds and reaction energetics\")\nprint(\"‚úì Training on full EXFOR allows transfer learning to specific isotopes\")\nprint(\"‚úó STILL can't guarantee smooth resonance curves\")\nprint(\"‚úó STILL poor extrapolation to unseen energy ranges\")\nprint(\"‚úó No explicit physics constraints (unitarity, conservation laws)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üü¢ Critical Insight #3: Features Matter, But Architecture Matters More\n",
    "\n",
    "Physics-aware features help XGBoost understand reactions better.\n",
    "\n",
    "**BUT** the fundamental problem remains:\n",
    "- Tree-based models are **piecewise constant**\n",
    "- No inductive bias for **smoothness**\n",
    "- No way to encode **physical constraints**\n",
    "\n",
    "---\n",
    "\n",
    "## Part 4: Feature Importance Analysis\n",
    "\n",
    "Let's see what XGBoost \"thinks\" is important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance\n",
    "importance_physics = xgb_physics.get_feature_importance()\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.barh(importance_physics['Feature'], importance_physics['Importance'])\n",
    "ax.set_xlabel('Importance (Gain)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('XGBoost Feature Importance (Physics Mode)', fontsize=14, fontweight='bold')\n",
    "ax.invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 5 Most Important Features:\")\n",
    "print(importance_physics.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### üéì Key Takeaway\n\n> **Low MSE on test data does NOT guarantee safe reactor predictions!**\n>\n> We need models that:\n> 1. Respect physics (smoothness, thresholds, unitarity)\n> 2. Extrapolate correctly (beyond training data)\n> 3. Prioritize safety-critical reactions (sensitivity weighting)\n> 4. **Handle data-sparse scenarios** (like Cl-35) without overfitting\n>\n> This is why we need **Physics-Informed Deep Learning**.\n\n---\n\n## Summary: Why Classical ML Fails\n\n| Issue | U-235 (Data-Rich) | Cl-35 (Data-Sparse) |\n|-------|-------------------|---------------------|\n| Staircase Effect | üî¥ Severe (even with lots of data) | üî¥ Severe |\n| Interpolation | üü° Approximate | üî¥ Very poor (large gaps) |\n| Extrapolation | üî¥ Fails | üî¥ Completely fails |\n| Physics Constraints | üî¥ None | üî¥ None |\n| Uncertainty Quantification | üî¥ Poor | üî¥ Very poor |\n| Training Speed | üü¢ Fast | üü¢ Fast |\n\n### The Path Forward\n\nWe need:\n1. **Graph Neural Networks** ‚Üí Learn nuclear topology (not just Z, A)\n2. **Transformers** ‚Üí Learn smooth energy sequences œÉ(E)\n3. **Physics-Informed Loss** ‚Üí Enforce unitarity, thresholds, conservation\n4. **Transfer Learning** ‚Üí Use U-235 knowledge to improve Cl-35 predictions\n5. **Uncertainty Quantification** ‚Üí Know when to trust sparse-data predictions\n\n---\n\n## Next Steps\n\nIn **Notebook 01**, we'll:\n- Build the **Chart of Nuclides as a Graph**\n- Visualize nuclear topology connecting U-235 and Cl-35\n- Understand how GNNs can transfer knowledge between isotopes\n\nIn **Notebook 02**, we'll:\n- Implement **GNN + Transformer**\n- Train on graph-structured real data\n- See **smooth, physics-compliant predictions** for both isotopes!\n\nIn **Notebook 03**, we'll:\n- Integrate with **OpenMC** for U-235 reactor validation\n- Achieve reactor-grade accuracy with real nuclear data\n- Demonstrate uncertainty quantification for Cl-35\n\nContinue to `01_Data_Fabric_and_Graph.ipynb` ‚Üí"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}