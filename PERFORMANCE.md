# NUCML-Next Performance Optimization Guide

## Loading Large EXFOR Databases (4.7GB+)

### Problem
Loading the full EXFOR database (4.7GB, ~18M measurements) can take 600+ seconds with default settings.

### Solutions

#### 1. Use Filters to Load Only What You Need (FASTEST) ‚ö°

If you're only working with specific isotopes or reactions, filter at load time:

```python
# Only load U-235 and Pu-239 fission and capture data
dataset = NucmlDataset(
    data_path='data/exfor_processed.parquet',
    mode='tabular',
    filters={
        'Z': [92, 94],           # Uranium and Plutonium
        'A': [235, 239],         # U-235 and Pu-239
        'MT': [18, 102]          # Fission and capture
    }
)
```

**Performance:**
- Filter pushdown happens in PyArrow (C++ speed)
- Only loads ~1-5% of data from disk
- Load time: **2-10 seconds** instead of 600 seconds
- Suitable for focused analysis, prototyping, debugging

#### 2. Optimized Full Load (Column Pruning + Memory Mapping)

The latest version includes automatic optimizations:

```python
# Loads full database with optimizations
dataset = NucmlDataset(
    data_path='data/exfor_processed.parquet',
    mode='tabular'
)
```

**Optimizations Applied:**
- ‚úÖ **Column pruning**: Only reads essential 10 columns (not all 50+)
- ‚úÖ **Memory mapping**: Faster I/O, less RAM pressure
- ‚úÖ **Multi-threaded read**: Parallel decompression
- ‚úÖ **Lazy graph building**: Graph built on first access

**Performance:**
- Column pruning: **50-60% faster** read time
- Memory mapping: **30-40% less RAM** usage
- Lazy graph: **Instant initialization**
- Expected load time: **60-120 seconds** (down from 600s)

#### 3. Lazy Loading for Interactive Development

```python
# Load only metadata initially, data on-demand
dataset = NucmlDataset(
    data_path='data/exfor_processed.parquet',
    mode='tabular',
    lazy_load=True  # Only loads 1000 rows initially
)
```

**Use case:**
- Rapid prototyping
- Testing code without waiting
- Exploring schema and data structure

#### 4. Pre-Filter with Partitioned Parquet (ADVANCED)

If you frequently work with specific subsets, re-export:

```python
# One-time: Export filtered data to new parquet
import pandas as pd
import pyarrow.parquet as pq

# Load with filter
table = pq.read_table(
    'data/exfor_processed.parquet',
    filters=[('Z', 'in', [92, 94]), ('MT', 'in', [18, 102, 16, 17])]
)

# Save subset
pq.write_table(table, 'data/actinides_fission.parquet')

# Future loads are instant
dataset = NucmlDataset('data/actinides_fission.parquet', mode='tabular')
```

**Performance:**
- Subset files: **100-500MB** instead of 4.7GB
- Load time: **1-5 seconds**
- Perfect for repeated workflows

---

## Performance Comparison

| Method | Load Time | RAM Usage | Use Case |
|--------|-----------|-----------|----------|
| Full load (old) | 600s | 8-12 GB | ‚ùå Too slow |
| **Optimized full load** | **60-120s** | **4-6 GB** | ‚úÖ Production training |
| **Filtered load** | **2-10s** | **200-800 MB** | ‚úÖ Focused analysis |
| Lazy load | <1s | <100 MB | ‚úÖ Prototyping |
| Pre-filtered file | 1-5s | 200-500 MB | ‚úÖ Repeated workflows |

---

## Recommended Workflow

### For Development/Prototyping:
```python
# Use filters to work with subset
dataset = NucmlDataset(
    data_path='data/exfor_processed.parquet',
    mode='tabular',
    filters={'Z': [92], 'MT': [18, 102]}  # Just U-235 fission/capture
)
```

### For Full Model Training:
```python
# Load full database with optimizations
dataset = NucmlDataset(
    data_path='data/exfor_processed.parquet',
    mode='tabular'
    # Automatic optimizations: column pruning, memory mapping, lazy graph
)

# Train on full data
df = dataset.to_tabular(mode='physics')
model.train(df)
```

### For GNN Training:
```python
# Graph mode with lazy building
dataset = NucmlDataset(
    data_path='data/exfor_processed.parquet',
    mode='graph'
    # Graph built on first access to get_global_graph()
)

# Graph builds when you need it
graph = dataset.get_global_graph()  # <-- Build happens here
```

---

## Monitoring Load Performance

The optimized loader provides timing breakdowns:

```
Loading data from data/exfor_processed.parquet...
  ‚Üí Parquet read: 45.2s (3.42 GB)
  ‚Üí Arrow‚ÜíPandas conversion: 23.1s
‚úì Loaded 18,345,672 EXFOR data points
```

**Interpretation:**
- **Parquet read**: I/O + decompression (benefits from column pruning, memory mapping)
- **Arrow‚ÜíPandas**: Type conversion (unavoidable, but faster with fewer columns)

If load time is still too slow:
1. Check disk speed (SSD vs HDD makes huge difference)
2. Use filters to reduce data size
3. Consider pre-filtering to create smaller files

---

## Advanced: PyArrow Native Operations (Future)

For maximum performance, work directly with PyArrow tables:

```python
import pyarrow.parquet as pq
import pyarrow.compute as pc

# Read as Arrow table (no pandas conversion)
table = pq.read_table(
    'data/exfor_processed.parquet',
    columns=['Z', 'A', 'Energy', 'CrossSection', 'MT'],
    memory_map=True
)

# Use PyArrow compute (C++ speed)
u235_mask = (table['Z'] == 92) & (table['A'] == 235)
u235_table = table.filter(u235_mask)

# Convert to pandas only when needed
df = u235_table.to_pandas()
```

This approach can be **2-5x faster** for large filtering operations.

---

## Summary: Best Practices

‚úÖ **DO:**
- Use filters for focused analysis
- Let optimizations run automatically (column pruning, memory mapping)
- Use lazy_load for prototyping
- Pre-filter to create smaller files for repeated workflows

‚ùå **DON'T:**
- Load full 4.7GB without filtering unless training production models
- Convert entire Arrow table to pandas if you don't need all columns
- Build graphs eagerly if you're in tabular mode

üéØ **Target Performance:**
- Filtered loads: **<10 seconds**
- Full optimized load: **60-120 seconds**
- Graph building: **30-60 seconds** (lazy, on-demand)
